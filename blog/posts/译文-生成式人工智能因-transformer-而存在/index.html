<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[译文] 生成式人工智能因 Transformer 而存在 | Bit by Bit</title>
<meta name=keywords content="LLM,transformer,自注意力机制"><meta name=description content="该文由 gemini-exp-126 翻译"><meta name=author content="Madhumita Murgia, FT 视觉叙事团队"><link rel=canonical href=https://viitetrix.github.io/blog/posts/%E8%AF%91%E6%96%87-%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9B%A0-transformer-%E8%80%8C%E5%AD%98%E5%9C%A8/><link crossorigin=anonymous href=/blog/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://viitetrix.github.io/blog/logo/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://viitetrix.github.io/blog/logo/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://viitetrix.github.io/blog/logo/favicon-32x32.png><link rel=apple-touch-icon href=https://viitetrix.github.io/blog/logo/apple-touch-icon.png><link rel=mask-icon href=https://viitetrix.github.io/blog/logo/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://viitetrix.github.io/blog/posts/%E8%AF%91%E6%96%87-%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9B%A0-transformer-%E8%80%8C%E5%AD%98%E5%9C%A8/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/jetbrains-mono@1.0.6/css/jetbrains-mono.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/misans@4.0.0/lib/Normal/MiSans-Regular.min.css><link rel=stylesheet href=//unpkg.com/heti/umd/heti.min.css><style>body{font-family:misans regular,sans-serif!important}.heti{font-family:misans regular,sans-serif!important}.heti code,.heti pre{font-family:jetbrains mono,misans regular,sans-serif!important}.heti{--heti-font-family:"MiSans Regular", sans-serif;--heti-font-family-code:"JetBrains Mono", "MiSans Regular", sans-serif;--heti-font-family-quote:"MiSans Regular", sans-serif;--heti-font-family-title:"MiSans Regular", sans-serif}.heti h1,.heti h2,.heti h3,.heti h4,.heti h5,.heti h6,.heti p{margin-bottom:1em}</style><meta property="og:url" content="https://viitetrix.github.io/blog/posts/%E8%AF%91%E6%96%87-%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9B%A0-transformer-%E8%80%8C%E5%AD%98%E5%9C%A8/"><meta property="og:site_name" content="Bit by Bit"><meta property="og:title" content="[译文] 生成式人工智能因 Transformer 而存在"><meta property="og:description" content="该文由 gemini-exp-126 翻译"><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-06T20:21:24+08:00"><meta property="article:modified_time" content="2025-01-06T20:21:24+08:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="自注意力机制"><meta name=twitter:card content="summary"><meta name=twitter:title content="[译文] 生成式人工智能因 Transformer 而存在"><meta name=twitter:description content="该文由 gemini-exp-126 翻译"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"文章","item":"https://viitetrix.github.io/blog/posts/"},{"@type":"ListItem","position":2,"name":"[译文] 生成式人工智能因 Transformer 而存在","item":"https://viitetrix.github.io/blog/posts/%E8%AF%91%E6%96%87-%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9B%A0-transformer-%E8%80%8C%E5%AD%98%E5%9C%A8/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[译文] 生成式人工智能因 Transformer 而存在","name":"[译文] 生成式人工智能因 Transformer 而存在","description":"该文由 gemini-exp-126 翻译","keywords":["LLM","transformer","自注意力机制"],"articleBody":"原文链接：Generative AI exists because of the transformer\n原文发布日期：2023 年 9 月 12日\n在过去的几年里，我们在构建智能机器这条长达数十年的探索之路上实现了一个巨大的飞跃：大语言模型的出现。\n这项基于人脑模拟研究的技术催生了一个被称为“生成式 AI”的新领域——这种软件能够创作出与人类水平相当的、可信且复杂的文本、图像和计算机代码。\n全球的企业已经开始探索这项新技术的应用，相信它将颠覆媒体、金融、法律和专业服务以及教育等公共服务领域。大语言模型基于被称为 transformer 模型的一项科学突破，它由谷歌研究人员在 2017 年提出。\n“虽然我们一直都清楚我们 transformer 研究的突破性意义，但几年后，我们对其在医疗保健、机器人技术和安全等新领域的持久潜力感到振奋，它增强了人类的创造力等等。”谷歌高级研究员 Slav Petrov 说道，他致力于构建包括大语言模型在内的 AI 智能体。\n大语言模型备受推崇的优势——通过编写和分析文本来提高生产力的能力——也是它对人类构成威胁的原因。根据高盛的说法，它可能使大型经济体中相当于 3 亿名全职员工面临自动化，导致大范围失业。\n随着这项技术迅速融入我们的生活，理解大语言模型如何生成文本意味着理解为什么这些模型是如此多才多艺的认知引擎——以及它们还能帮助创造什么。\n要编写文本，大语言模型必须首先将单词转换成它们能理解的语言。\n首先，一段文字会被分解成 token ——这是可以被编码的基本单元。虽然 token 通常表示单词的一部分，但在本例中，为了简单起见，我们把每个完整的单词都当作一个 token。\n为了理解一个词的含义，以我们例子中的 work 为例，大语言模型首先会在海量的训练数据中观察它的上下文，并注意它的邻近词 。这些数据集基于互联网上发布的文本整理而成，新的大语言模型使用了数十亿个单词进行训练。\n最终，我们会得到一个庞大的单词集合，其中既包括在训练数据中与 work 一同出现的词，也包括那些没有和它一起出现的词。\n当大语言模型 处理 这些单词时，会生成一个向量（也就是一个数值列表），并根据每个单词在训练数据中与 work 的邻近程度来调整这个向量。这个向量被称为词嵌入。\n词嵌入可以包含数百个数值，每个数值代表单词含义的某个不同方面。就像你可以通过房子的特征——类型、位置、卧室数量、浴室数量、楼层数——来描述它一样，词嵌入中的数值量化了一个单词的语言特征。\n由于这些特征的提取方式，我们并不确切知道每个数值代表什么含义，但那些我们预期会以类似方式使用的单词，通常会有相似的词嵌入。\n例如，sea 和 ocean 这两个词可能不会在完全相同的上下文中使用（“all at ocean”不能直接替代“all at sea”，后者表示“不知所措”），但它们的含义非常相近，而词嵌入可以让我们量化这种相近的程度\n通过把每个词嵌入所代表的数百个数值缩减到只有两个，我们就能更清楚地看到这些单词之间的距离。\n我们可能会发现一些 代词 或 交通方式 的集群，而能够以这种方式量化单词，正是大语言模型生成文本的第一步。\n但这并不是大语言模型如此聪明的全部原因。真正让它们能够如此流畅地解析和写作的，是一种名为 transformer 的工具。它从根本上加速并增强了计算机理解语言的方式。\nTransformer 可以一次性处理整个序列 —— 无论是句子、段落还是整篇文章 —— 分析其所有组成部分，而不仅仅是单个词汇。\n这使得软件能够更好地捕捉上下文和模式，并更准确地翻译或生成文本。这种同步处理还使得大语言模型的训练速度更快，从而提高了它们的效率和扩展能力。\n2017 年 6 月，谷歌的八位 AI 研究人员首次发表了概述 transformer 模型的论文。他们这份 11 页的研究论文标志着生成式 AI 时代的开始。\nTransformer 架构的一个关键概念是自注意力机制。这使得 LLM 能够理解单词之间的关系。\n自注意力机制查看一段文本中的每个 token ，并判断哪些 token 对于理解其含义最重要。\n在 transformer 之前，最先进的 AI 翻译方法是循环神经网络 (RNNs)，它会扫描句子中的每个单词并按顺序处理。\n通过自注意力机制，transformer 可以同时计算句子中的所有单词。捕捉这种上下文使大语言模型具备了更复杂的语言解析能力。\n在这个例子中，通过一次性评估整个句子，transformer 能够理解 interest 在这里是用作名词，表示个人对政治的兴趣。\n如果我们调整一下句子\n模型会理解 interest 现在被用作金融意义上的“利息”。\n当我们将两个句子组合在一起时，由于模型对伴随文本的关注，它仍然能够识别每个单词的正确含义。\n在第一种用法中，模型最关注的是 no 和 in 。\n而在第二种用法中，模型最关注的是 rate 和 bank 。\n这个功能对于高级文本生成至关重要。如果没有它，在某些上下文中可以互换但在其他上下文中不能互换的单词可能会被错误地使用。\n简而言之，自注意力确保了在总结关于利率的句子时，不会误用 热情 一词。\n这种能力不仅仅局限于像“interest”这样具有多重含义的单词。\n在下面的句子中，自注意力机制能够计算出 it 最有可能指的是 dog 。\n如果我们改变句子，将 hungry 换成 delicious ，模型能够重新计算， it 现在最有可能指的是 bone 。\n自注意力机制对语言处理的好处随着规模的扩大而增加。它允许 LLM 从句子边界之外获取 上下文 ，使模型更好地理解单词的使用方式和时间。\nGPT-4 是全球最大且最先进的大语言模型之一，是 OpenAI 最新的 AI 模型。该公司称，该模型在美国律师资格考试、大学预修课程（AP）考试和 SAT 学术能力评估测试等多个学术和专业基准测试中展现出“人类水平的表现”。\nGPT-4 可以生成和接收大量文本：用户可以输入多达 25,000 个英文单词，这意味着它可以处理详细的财务文件、文学作品或技术手册。\n该产品的出现重塑了整个科技行业，包括谷歌、Meta 和微软这些支持 OpenAI 的科技巨头，以及一些小型初创公司，都在竞相抢占这一领域的领先地位。\n他们发布的知名大语言模型包括：谷歌的 PaLM 模型（为其聊天机器人 Bard 提供支持）、Anthropic 的 Claude 模型、Meta 的 LLaMA 模型，以及 Cohere 的 Command 模型等。\n虽然这些模型已经被许多企业采用，但其背后的一些公司正面临着围绕其使用从网络上抓取的受版权保护的文本、图像和音频的法律纠纷 。\n其原因在于，当前的大语言模型几乎都是基于整个英语互联网进行训练的——这使得它们比前几代模型强大得多。\n从这个庞大的词语和图像语料库中，这些模型学习如何识别模式，并最终预测下一个最佳词语。\n简单来说，在对提示进行 token 化和编码后，我们就得到一个数据块，它以机器能理解的方式表示我们的输入，包括词语之间的含义、位置和关系。\n该模型现在的目标是预测序列中的下一个单词，并重复执行此操作，直到生成完整的文本输出。\n为此，该模型会为每个 token 分配一个 概率分数，表示其成为序列中下一个单词的可能性。\n它会持续执行此操作，直到生成完整的文本输出。\n但是，这种孤立地预测下一个单词的方法（称为“贪婪搜索”）可能会产生问题。有时，尽管每个单独的 token 可能是下一个最佳匹配，但整个短语的相关性可能会降低。\n不一定总是错误的，但可能也不是你所期望的。\nTransformer 使用多种方法来解决这个问题并提高其输出质量。其中一个例子叫做 集束搜索 。\n它不只关注序列中的下一个单词，而是将更大范围的 token 集合作为一个整体来考虑其概率。\n通过集束搜索，该模型能够考虑多条路径并找到最佳选项。\n这产生了更好的结果，最终生成更连贯、更像人类的文本。\n但是事情并不总按计划进行。虽然文本可能看起来合理且连贯，但它并不总是与事实相符。大语言模型不是查找事实的搜索引擎；它们是模式识别引擎，猜测序列中的下一个最佳选项。\n由于这种固有的预测性质，大语言模型还可以在研究人员称之为“幻觉”的过程中捏造信息。它们可以生成虚构的数字、姓名、日期、引言——甚至是网络链接或整篇文章。\n大语言模型的使用者分享了一些示例。其中包括指向《金融时报》和彭博社不存在的新闻文章的链接，对研究论文的虚假引用，已出版书籍的错误作者，以及充满事实错误的传记。\n在纽约发生的一起引人注目的事件中，一位律师使用 ChatGPT 为一个案件撰写了一份简报。当辩方质疑这份报告时，他们发现其中充斥着虚构的司法意见和法律引文。“我不知道 ChatGPT 会编造案例，”这位律师后来在自己的法庭听证会上告诉法官。\n尽管研究人员表示幻觉永远不会被完全消除，但 Google、OpenAI 和其他公司正在努力通过一个称为“溯源”的过程来限制它们。这包括将大语言模型的输出与网络搜索结果进行交叉检查，并向用户提供引文以便他们进行验证。\n人类也被用来提供反馈和填补信息空白——这个过程被称为基于人类反馈的强化学习 (RLHF)——这进一步提高了输出的质量。但是，了解哪些查询可能会触发这些幻觉，以及如何预测和减少它们，仍然是一个巨大的研究挑战。\n尽管存在这些局限性，transformer 还是催生了大量尖端 AI 应用。除了为 Bard 和 ChatGPT 等聊天机器人提供支持外，它还驱动我们手机键盘上的自动完成和智能扬声器中的语音识别。\n然而，它真正的力量在于语言之外。它的发明者发现，transformer 模型可以识别和预测任何重复的主题或模式。从图像中的像素（使用 Dall-E、Midjourney 和 Stable Diffusion 等工具），到使用 GitHub CoPilot 等生成器的计算机代码。它甚至可以预测音乐中的音符和蛋白质中的 DNA，以帮助设计药物分子。\n几十年来，研究人员构建了专门的模型来总结、翻译、搜索和检索。transformer 将所有这些操作统一到一个单一的结构中，能够执行各种各样的任务。\n“采用这个预测下一个单词的简单模型，它……可以做任何事情，”AI 初创公司 Cohere 的首席执行官兼 transformer 论文的合著者 Aidan Gomez 说。\n现在，他们有一种“在整个互联网上训练”的模型，“输出的结果可以完成所有这些工作，并且比以前的任何东西都好”，他说。\n“这是这个故事的神奇之处。”\n","wordCount":"3814","inLanguage":"zh","datePublished":"2025-01-06T20:21:24+08:00","dateModified":"2025-01-06T20:21:24+08:00","author":[{"@type":"Person","name":"Madhumita Murgia"},{"@type":"Person","name":"FT 视觉叙事团队"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://viitetrix.github.io/blog/posts/%E8%AF%91%E6%96%87-%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9B%A0-transformer-%E8%80%8C%E5%AD%98%E5%9C%A8/"},"publisher":{"@type":"Organization","name":"Bit by Bit","logo":{"@type":"ImageObject","url":"https://viitetrix.github.io/blog/logo/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://viitetrix.github.io/blog/ accesskey=h title="Bit by Bit (Alt + H)"><img src=https://viitetrix.github.io/blog/logo/apple-touch-icon.png alt aria-label=logo height=40>Bit by Bit</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://viitetrix.github.io/blog/ title=主页><span>主页</span></a></li><li><a href=https://viitetrix.github.io/blog/posts title=文章><span>文章</span></a></li><li><a href=https://viitetrix.github.io/blog/archives title=归档><span>归档</span></a></li><li><a href=https://viitetrix.github.io/blog/categories title=分类><span>分类</span></a></li><li><a href=https://viitetrix.github.io/blog/tags title=标签><span>标签</span></a></li><li><a href=https://viitetrix.github.io/blog/search title="搜索 (Alt + /)" accesskey=/><span>搜索</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://viitetrix.github.io/blog/>首页</a>&nbsp;»&nbsp;<a href=https://viitetrix.github.io/blog/posts/>文章</a></div><h1 class="post-title entry-hint-parent">[译文] 生成式人工智能因 Transformer 而存在</h1><div class=post-description>该文由 gemini-exp-126 翻译</div><div class=post-meta>创建: <span title='2025-01-06 20:21:24 +0800 CST'>2025-01-06</span> | 更新: 2025-01-06 | 字数: 3814字 | 时长: 8分钟 | 作者: Madhumita Murgia, FT 视觉叙事团队</div></header><div class="post-content heti heti--ancient"><p><strong>原文链接</strong>：<a href=https://ig.ft.com/generative-ai/>Generative AI exists because of the transformer</a></p><p><strong>原文发布日期</strong>：2023 年 9 月 12日</p><hr><p><img alt=1 loading=lazy src=/blog/img/20250106/1.gif></p><p>在过去的几年里，我们在构建智能机器这条长达数十年的探索之路上实现了一个巨大的飞跃：大语言模型的出现。</p><p>这项基于人脑模拟研究的技术催生了一个被称为“生成式 AI”的新领域——这种软件能够创作出与人类水平相当的、可信且复杂的文本、图像和计算机代码。</p><p>全球的企业已经开始探索这项新技术的应用，相信它将颠覆媒体、金融、法律和专业服务以及教育等公共服务领域。大语言模型基于被称为 transformer 模型的一项科学突破，它由谷歌研究人员在 2017 年提出。</p><p>“虽然我们一直都清楚我们 transformer 研究的突破性意义，但几年后，我们对其在医疗保健、机器人技术和安全等新领域的持久潜力感到振奋，它增强了人类的创造力等等。”谷歌高级研究员 Slav Petrov 说道，他致力于构建包括大语言模型在内的 AI 智能体。</p><p>大语言模型备受推崇的优势——通过编写和分析文本来提高生产力的能力——也是它对人类构成威胁的原因。根据高盛的说法，它可能使大型经济体中相当于 3 亿名全职员工面临自动化，导致大范围失业。</p><p>随着这项技术迅速融入我们的生活，理解大语言模型如何生成文本意味着理解为什么这些模型是如此多才多艺的认知引擎——以及它们还能帮助创造什么。</p><br><p>要编写文本，大语言模型必须首先将单词转换成它们能理解的语言。</p><p>首先，一段文字会被分解成 <code>token</code> ——这是可以被编码的基本单元。虽然 token 通常表示单词的一部分，但在本例中，为了简单起见，我们把每个完整的单词都当作一个 token。</p><p><img alt=2 loading=lazy src=/blog/img/20250106/2.gif></p><p>为了理解一个词的含义，以我们例子中的 <code>work</code> 为例，大语言模型首先会在海量的训练数据中观察它的上下文，并注意它的<code>邻近词</code> 。这些数据集基于互联网上发布的文本整理而成，新的大语言模型使用了数十亿个单词进行训练。</p><p><img alt=3 loading=lazy src=/blog/img/20250106/3.gif></p><p>最终，我们会得到一个庞大的单词集合，其中既包括在训练数据中与 <code>work</code> 一同出现的词，也包括那些没有和它一起出现的词。</p><p><img alt=4 loading=lazy src=/blog/img/20250106/4.jpg></p><p>当大语言模型 <code>处理</code> 这些单词时，会生成一个向量（也就是一个数值列表），并根据每个单词在训练数据中与 <code>work</code> 的邻近程度来调整这个向量。这个向量被称为词嵌入。</p><p><img alt=5 loading=lazy src=/blog/img/20250106/5.gif></p><p>词嵌入可以包含数百个数值，每个数值代表单词含义的某个不同方面。就像你可以通过房子的特征——类型、位置、卧室数量、浴室数量、楼层数——来描述它一样，词嵌入中的数值量化了一个单词的语言特征。</p><p><img alt=6 loading=lazy src=/blog/img/20250106/6.jpg></p><p>由于这些特征的提取方式，我们并不确切知道每个数值代表什么含义，但那些我们预期会以类似方式使用的单词，通常会有相似的词嵌入。</p><p>例如，<code>sea</code> 和 <code>ocean</code> 这两个词可能不会在完全相同的上下文中使用（“all at ocean”不能直接替代“all at sea”，后者表示“不知所措”），但它们的含义非常相近，而词嵌入可以让我们量化这种相近的程度</p><p><img alt=7 loading=lazy src=/blog/img/20250106/7.jpg></p><p>通过把每个词嵌入所代表的数百个数值缩减到只有两个，我们就能更清楚地看到这些单词之间的距离。</p><p><img alt=8 loading=lazy src=/blog/img/20250106/8.jpg></p><p>我们可能会发现一些 <code>代词</code> 或 <code>交通方式</code> 的集群，而能够以这种方式量化单词，正是大语言模型生成文本的第一步。</p><p><img alt=9 loading=lazy src=/blog/img/20250106/9.jpg></p><br><p>但这并不是大语言模型如此聪明的全部原因。真正让它们能够如此流畅地解析和写作的，是一种名为 transformer 的工具。它从根本上加速并增强了计算机理解语言的方式。</p><p>Transformer 可以一次性处理整个序列 —— 无论是句子、段落还是整篇文章 —— 分析其所有组成部分，而不仅仅是单个词汇。</p><p>这使得软件能够更好地捕捉上下文和模式，并更准确地翻译或生成文本。这种同步处理还使得大语言模型的训练速度更快，从而提高了它们的效率和扩展能力。</p><p>2017 年 6 月，<a href=https://www.ft.com/content/37bb01af-ee46-4483-982f-ef3921436a50>谷歌的八位 AI 研究人员</a>首次发表了概述 transformer 模型的<a href=https://research.google/blog/transformer-a-novel-neural-network-architecture-for-language-understanding/>论文</a>。他们这份 11 页的研究论文标志着生成式 AI 时代的开始。</p><br><p>Transformer 架构的一个关键概念是自注意力机制。这使得 LLM 能够理解单词之间的关系。</p><p>自注意力机制查看一段文本中的每个 <code>token</code> ，并判断哪些 token 对于理解其含义最重要。</p><p><img alt=10 loading=lazy src=/blog/img/20250106/10.jpg></p><p>在 transformer 之前，最先进的 AI 翻译方法是循环神经网络 (RNNs)，它会扫描句子中的每个单词并按顺序处理。</p><p><img alt=11 loading=lazy src=/blog/img/20250106/11.gif></p><p>通过自注意力机制，transformer 可以同时计算句子中的所有单词。捕捉这种上下文使大语言模型具备了更复杂的语言解析能力。</p><p><img alt=12 loading=lazy src=/blog/img/20250106/12.jpg></p><p>在这个例子中，通过一次性评估整个句子，transformer 能够理解 <code>interest</code> 在这里是用作名词，表示个人对政治的兴趣。</p><p><img alt=13 loading=lazy src=/blog/img/20250106/13.jpg></p><p>如果我们调整一下句子</p><p>模型会理解 <code>interest</code> 现在被用作金融意义上的“利息”。</p><p><img alt=14 loading=lazy src=/blog/img/20250106/14.gif></p><p>当我们将两个句子组合在一起时，由于模型对伴随文本的关注，它仍然能够识别每个单词的正确含义。</p><p>在第一种用法中，模型最关注的是 <code>no</code> 和 <code>in</code> 。</p><p>而在第二种用法中，模型最关注的是 <code>rate</code> 和 <code>bank</code> 。</p><p><img alt=15 loading=lazy src=/blog/img/20250106/15.gif></p><p>这个功能对于高级文本生成至关重要。如果没有它，在某些上下文中可以互换但在其他上下文中不能互换的单词可能会被错误地使用。</p><p><img alt=16 loading=lazy src=/blog/img/20250106/16.jpg></p><p>简而言之，自注意力确保了在总结关于利率的句子时，不会误用 <code>热情</code> 一词。</p><p><img alt=17 loading=lazy src=/blog/img/20250106/17.jpg></p><p>这种能力不仅仅局限于像“interest”这样具有多重含义的单词。</p><br><p>在下面的句子中，自注意力机制能够计算出 <code>it</code> 最有可能指的是 <code>dog</code> 。</p><p><img alt=18 loading=lazy src=/blog/img/20250106/18.jpg></p><p>如果我们改变句子，将 <code>hungry</code> 换成 <code>delicious</code> ，模型能够重新计算， <code>it</code> 现在最有可能指的是 <code>bone</code> 。</p><p><img alt=19 loading=lazy src=/blog/img/20250106/19.jpg></p><p>自注意力机制对语言处理的好处随着规模的扩大而增加。它允许 LLM 从句子边界之外获取 <code>上下文</code> ，使模型更好地理解单词的使用方式和时间。</p><p><img alt=20 loading=lazy src=/blog/img/20250106/20.gif></p><br><p>GPT-4 是全球最大且最先进的大语言模型之一，是 OpenAI 最新的 AI 模型。该公司称，该模型在美国律师资格考试、大学预修课程（AP）考试和 SAT 学术能力评估测试等多个学术和专业基准测试中展现出“人类水平的表现”。</p><p>GPT-4 可以生成和接收大量文本：用户可以输入多达 25,000 个英文单词，这意味着它可以处理详细的财务文件、文学作品或技术手册。</p><p>该产品的出现重塑了整个科技行业，包括谷歌、Meta 和微软这些支持 OpenAI 的科技巨头，以及一些小型初创公司，都在竞相抢占这一领域的领先地位。</p><p>他们发布的知名大语言模型包括：谷歌的 PaLM 模型（为其聊天机器人 Bard 提供支持）、Anthropic 的 Claude 模型、Meta 的 LLaMA 模型，以及 Cohere 的 Command 模型等。</p><p>虽然这些模型已经被许多企业采用，但其背后的一些公司正面临着围绕其使用从网络上抓取的受版权保护的文本、图像和音频的<a href=https://www.ft.com/content/704d0bba-2653-4a27-bee1-ee45c6ed1080>法律纠纷</a> 。</p><p>其原因在于，当前的大语言模型几乎都是基于整个英语互联网进行训练的——这使得它们比前几代模型强大得多。</p><p>从这个庞大的词语和图像语料库中，这些模型学习如何识别模式，并最终预测下一个最佳词语。</p><br><p>简单来说，在对提示进行 token 化和编码后，我们就得到一个数据块，它以机器能理解的方式表示我们的输入，包括词语之间的含义、位置和关系。</p><p><img alt=21 loading=lazy src=/blog/img/20250106/21.jpg></p><p>该模型现在的目标是预测序列中的下一个单词，并重复执行此操作，直到生成完整的文本输出。</p><p>为此，该模型会为每个 token 分配一个 <code>概率分数</code>，表示其成为序列中下一个单词的可能性。</p><p>它会持续执行此操作，直到生成完整的文本输出。</p><p>但是，这种孤立地预测下一个单词的方法（称为“贪婪搜索”）可能会产生问题。有时，尽管每个单独的 token 可能是下一个最佳匹配，但整个短语的相关性可能会降低。</p><p>不一定总是错误的，但可能也不是你所期望的。</p><p><img alt=22 loading=lazy src=/blog/img/20250106/22.gif></p><p>Transformer 使用多种方法来解决这个问题并提高其输出质量。其中一个例子叫做 <code>集束搜索</code> 。</p><p>它不只关注序列中的下一个单词，而是将更大范围的 token 集合作为一个整体来考虑其概率。</p><p>通过集束搜索，该模型能够考虑多条路径并找到最佳选项。</p><p>这产生了更好的结果，最终生成更连贯、更像人类的文本。</p><p><img alt=23 loading=lazy src=/blog/img/20250106/23.gif></p><br><p>但是事情并不总按计划进行。虽然文本可能看起来合理且连贯，但它并不总是与事实相符。大语言模型不是查找事实的搜索引擎；它们是模式识别引擎，猜测序列中的下一个最佳选项。</p><p>由于这种固有的预测性质，大语言模型还可以在研究人员称之为“幻觉”的过程中捏造信息。它们可以生成虚构的数字、姓名、日期、引言——甚至是网络链接或整篇文章。</p><p>大语言模型的使用者分享了一些示例。其中包括指向《金融时报》和彭博社不存在的新闻文章的链接，对研究论文的虚假引用，已出版书籍的错误作者，以及充满事实错误的传记。</p><p>在纽约发生的一起引人注目的<a href=https://www.nytimes.com/2023/06/08/nyregion/lawyer-chatgpt-sanctions.html>事件</a>中，一位律师使用 ChatGPT 为一个案件撰写了一份简报。当辩方质疑这份报告时，他们发现其中充斥着虚构的司法意见和法律引文。“我不知道 ChatGPT 会编造案例，”这位律师后来在自己的法庭听证会上告诉法官。</p><p>尽管研究人员表示幻觉永远不会被完全消除，但 Google、OpenAI 和其他公司正在努力通过一个称为“溯源”的过程来限制它们。这包括将大语言模型的输出与网络搜索结果进行交叉检查，并向用户提供引文以便他们进行验证。</p><p>人类也被用来提供反馈和填补信息空白——这个过程被称为基于人类反馈的强化学习 (RLHF)——这进一步提高了输出的质量。但是，了解哪些查询可能会触发这些幻觉，以及如何预测和减少它们，仍然是一个巨大的研究挑战。</p><p>尽管存在这些局限性，transformer 还是催生了大量尖端 AI 应用。除了为 Bard 和 ChatGPT 等聊天机器人提供支持外，它还驱动我们手机键盘上的自动完成和智能扬声器中的语音识别。</p><p>然而，它真正的力量在于语言之外。它的发明者发现，transformer 模型可以识别和预测任何重复的主题或模式。从图像中的像素（使用 Dall-E、Midjourney 和 Stable Diffusion 等工具），到使用 GitHub CoPilot 等生成器的计算机代码。它甚至可以预测音乐中的音符和蛋白质中的 DNA，以<a href=https://www.ft.com/content/dd557790-a39b-4fd1-8f18-c73532a61b3e>帮助设计</a>药物分子。</p><p>几十年来，研究人员构建了专门的模型来总结、翻译、搜索和检索。transformer 将所有这些操作统一到一个单一的结构中，能够执行各种各样的任务。</p><p>“采用这个预测下一个单词的简单模型，它……可以做任何事情，”AI 初创公司 Cohere 的首席执行官兼 transformer 论文的合著者 Aidan Gomez 说。</p><p>现在，他们有一种“在整个互联网上训练”的模型，“输出的结果可以完成所有这些工作，并且比以前的任何东西都好”，他说。</p><p>“这是这个故事的神奇之处。”</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://viitetrix.github.io/blog/tags/llm/>LLM</a></li><li><a href=https://viitetrix.github.io/blog/tags/transformer/>Transformer</a></li><li><a href=https://viitetrix.github.io/blog/tags/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/>自注意力机制</a></li></ul><nav class=paginav><a class=next href=https://viitetrix.github.io/blog/posts/%E8%AF%91%E6%96%87-chatgpt%E4%BA%92%E8%81%94%E7%BD%91%E7%9A%84%E6%A8%A1%E7%B3%8A%E7%BC%A9%E5%BD%B1/><span class=title>»</span><br><span>[译文] ChatGPT：互联网的模糊缩影</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://viitetrix.github.io/blog/>Bit by Bit</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script src=//unpkg.com/heti/umd/heti-addon.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){const e=new Heti(".heti");e.autoSpacing()})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script src=//unpkg.com/heti/umd/heti-addon.min.js></script><script>window.addEventListener("load",function(){requestAnimationFrame(function(){const e=document.querySelectorAll(".heti");if(e.length>0){const e=new Heti(".heti");e.autoSpacing(),console.log("Heti initialized")}})})</script></body></html>