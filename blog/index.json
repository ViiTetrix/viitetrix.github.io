[{"content":"原文链接：Generative AI exists because of the transformer\n原文发布日期：2023 年 9 月 12日\n在过去的几年里，我们在构建智能机器这条长达数十年的探索之路上实现了一个巨大的飞跃：大语言模型的出现。\n这项基于人脑模拟研究的技术催生了一个被称为“生成式 AI”的新领域——这种软件能够创作出与人类水平相当的、可信且复杂的文本、图像和计算机代码。\n全球的企业已经开始探索这项新技术的应用，相信它将颠覆媒体、金融、法律和专业服务以及教育等公共服务领域。大语言模型基于被称为 transformer 模型的一项科学突破，它由谷歌研究人员在 2017 年提出。\n“虽然我们一直都清楚我们 transformer 研究的突破性意义，但几年后，我们对其在医疗保健、机器人技术和安全等新领域的持久潜力感到振奋，它增强了人类的创造力等等。”谷歌高级研究员 Slav Petrov 说道，他致力于构建包括大语言模型在内的 AI 智能体。\n大语言模型备受推崇的优势——通过编写和分析文本来提高生产力的能力——也是它对人类构成威胁的原因。根据高盛的说法，它可能使大型经济体中相当于 3 亿名全职员工面临自动化，导致大范围失业。\n随着这项技术迅速融入我们的生活，理解大语言模型如何生成文本意味着理解为什么这些模型是如此多才多艺的认知引擎——以及它们还能帮助创造什么。\n要编写文本，大语言模型必须首先将单词转换成它们能理解的语言。\n首先，一段文字会被分解成 token ——这是可以被编码的基本单元。虽然 token 通常表示单词的一部分，但在本例中，为了简单起见，我们把每个完整的单词都当作一个 token。\n为了理解一个词的含义，以我们例子中的 work 为例，大语言模型首先会在海量的训练数据中观察它的上下文，并注意它的邻近词 。这些数据集基于互联网上发布的文本整理而成，新的大语言模型使用了数十亿个单词进行训练。\n最终，我们会得到一个庞大的单词集合，其中既包括在训练数据中与 work 一同出现的词，也包括那些没有和它一起出现的词。\n当大语言模型 处理 这些单词时，会生成一个向量（也就是一个数值列表），并根据每个单词在训练数据中与 work 的邻近程度来调整这个向量。这个向量被称为词嵌入。\n词嵌入可以包含数百个数值，每个数值代表单词含义的某个不同方面。就像你可以通过房子的特征——类型、位置、卧室数量、浴室数量、楼层数——来描述它一样，词嵌入中的数值量化了一个单词的语言特征。\n由于这些特征的提取方式，我们并不确切知道每个数值代表什么含义，但那些我们预期会以类似方式使用的单词，通常会有相似的词嵌入。\n例如，sea 和 ocean 这两个词可能不会在完全相同的上下文中使用（“all at ocean”不能直接替代“all at sea”，后者表示“不知所措”），但它们的含义非常相近，而词嵌入可以让我们量化这种相近的程度\n通过把每个词嵌入所代表的数百个数值缩减到只有两个，我们就能更清楚地看到这些单词之间的距离。\n我们可能会发现一些 代词 或 交通方式 的集群，而能够以这种方式量化单词，正是大语言模型生成文本的第一步。\n但这并不是大语言模型如此聪明的全部原因。真正让它们能够如此流畅地解析和写作的，是一种名为 transformer 的工具。它从根本上加速并增强了计算机理解语言的方式。\nTransformer 可以一次性处理整个序列 —— 无论是句子、段落还是整篇文章 —— 分析其所有组成部分，而不仅仅是单个词汇。\n这使得软件能够更好地捕捉上下文和模式，并更准确地翻译或生成文本。这种同步处理还使得大语言模型的训练速度更快，从而提高了它们的效率和扩展能力。\n2017 年 6 月，谷歌的八位 AI 研究人员首次发表了概述 transformer 模型的论文。他们这份 11 页的研究论文标志着生成式 AI 时代的开始。\nTransformer 架构的一个关键概念是自注意力机制。这使得 LLM 能够理解单词之间的关系。\n自注意力机制查看一段文本中的每个 token ，并判断哪些 token 对于理解其含义最重要。\n在 transformer 之前，最先进的 AI 翻译方法是循环神经网络 (RNNs)，它会扫描句子中的每个单词并按顺序处理。\n通过自注意力机制，transformer 可以同时计算句子中的所有单词。捕捉这种上下文使大语言模型具备了更复杂的语言解析能力。\n在这个例子中，通过一次性评估整个句子，transformer 能够理解 interest 在这里是用作名词，表示个人对政治的兴趣。\n如果我们调整一下句子\n模型会理解 interest 现在被用作金融意义上的“利息”。\n当我们将两个句子组合在一起时，由于模型对伴随文本的关注，它仍然能够识别每个单词的正确含义。\n在第一种用法中，模型最关注的是 no 和 in 。\n而在第二种用法中，模型最关注的是 rate 和 bank 。\n这个功能对于高级文本生成至关重要。如果没有它，在某些上下文中可以互换但在其他上下文中不能互换的单词可能会被错误地使用。\n简而言之，自注意力确保了在总结关于利率的句子时，不会误用 热情 一词。\n这种能力不仅仅局限于像“interest”这样具有多重含义的单词。\n在下面的句子中，自注意力机制能够计算出 it 最有可能指的是 dog 。\n如果我们改变句子，将 hungry 换成 delicious ，模型能够重新计算， it 现在最有可能指的是 bone 。\n自注意力机制对语言处理的好处随着规模的扩大而增加。它允许 LLM 从句子边界之外获取 上下文 ，使模型更好地理解单词的使用方式和时间。\nGPT-4 是全球最大且最先进的大语言模型之一，是 OpenAI 最新的 AI 模型。该公司称，该模型在美国律师资格考试、大学预修课程（AP）考试和 SAT 学术能力评估测试等多个学术和专业基准测试中展现出“人类水平的表现”。\nGPT-4 可以生成和接收大量文本：用户可以输入多达 25,000 个英文单词，这意味着它可以处理详细的财务文件、文学作品或技术手册。\n该产品的出现重塑了整个科技行业，包括谷歌、Meta 和微软这些支持 OpenAI 的科技巨头，以及一些小型初创公司，都在竞相抢占这一领域的领先地位。\n他们发布的知名大语言模型包括：谷歌的 PaLM 模型（为其聊天机器人 Bard 提供支持）、Anthropic 的 Claude 模型、Meta 的 LLaMA 模型，以及 Cohere 的 Command 模型等。\n虽然这些模型已经被许多企业采用，但其背后的一些公司正面临着围绕其使用从网络上抓取的受版权保护的文本、图像和音频的法律纠纷 。\n其原因在于，当前的大语言模型几乎都是基于整个英语互联网进行训练的——这使得它们比前几代模型强大得多。\n从这个庞大的词语和图像语料库中，这些模型学习如何识别模式，并最终预测下一个最佳词语。\n简单来说，在对提示进行 token 化和编码后，我们就得到一个数据块，它以机器能理解的方式表示我们的输入，包括词语之间的含义、位置和关系。\n该模型现在的目标是预测序列中的下一个单词，并重复执行此操作，直到生成完整的文本输出。\n为此，该模型会为每个 token 分配一个 概率分数，表示其成为序列中下一个单词的可能性。\n它会持续执行此操作，直到生成完整的文本输出。\n但是，这种孤立地预测下一个单词的方法（称为“贪婪搜索”）可能会产生问题。有时，尽管每个单独的 token 可能是下一个最佳匹配，但整个短语的相关性可能会降低。\n不一定总是错误的，但可能也不是你所期望的。\nTransformer 使用多种方法来解决这个问题并提高其输出质量。其中一个例子叫做 集束搜索 。\n它不只关注序列中的下一个单词，而是将更大范围的 token 集合作为一个整体来考虑其概率。\n通过集束搜索，该模型能够考虑多条路径并找到最佳选项。\n这产生了更好的结果，最终生成更连贯、更像人类的文本。\n但是事情并不总按计划进行。虽然文本可能看起来合理且连贯，但它并不总是与事实相符。大语言模型不是查找事实的搜索引擎；它们是模式识别引擎，猜测序列中的下一个最佳选项。\n由于这种固有的预测性质，大语言模型还可以在研究人员称之为“幻觉”的过程中捏造信息。它们可以生成虚构的数字、姓名、日期、引言——甚至是网络链接或整篇文章。\n大语言模型的使用者分享了一些示例。其中包括指向《金融时报》和彭博社不存在的新闻文章的链接，对研究论文的虚假引用，已出版书籍的错误作者，以及充满事实错误的传记。\n在纽约发生的一起引人注目的事件中，一位律师使用 ChatGPT 为一个案件撰写了一份简报。当辩方质疑这份报告时，他们发现其中充斥着虚构的司法意见和法律引文。“我不知道 ChatGPT 会编造案例，”这位律师后来在自己的法庭听证会上告诉法官。\n尽管研究人员表示幻觉永远不会被完全消除，但 Google、OpenAI 和其他公司正在努力通过一个称为“溯源”的过程来限制它们。这包括将大语言模型的输出与网络搜索结果进行交叉检查，并向用户提供引文以便他们进行验证。\n人类也被用来提供反馈和填补信息空白——这个过程被称为基于人类反馈的强化学习 (RLHF)——这进一步提高了输出的质量。但是，了解哪些查询可能会触发这些幻觉，以及如何预测和减少它们，仍然是一个巨大的研究挑战。\n尽管存在这些局限性，transformer 还是催生了大量尖端 AI 应用。除了为 Bard 和 ChatGPT 等聊天机器人提供支持外，它还驱动我们手机键盘上的自动完成和智能扬声器中的语音识别。\n然而，它真正的力量在于语言之外。它的发明者发现，transformer 模型可以识别和预测任何重复的主题或模式。从图像中的像素（使用 Dall-E、Midjourney 和 Stable Diffusion 等工具），到使用 GitHub CoPilot 等生成器的计算机代码。它甚至可以预测音乐中的音符和蛋白质中的 DNA，以帮助设计药物分子。\n几十年来，研究人员构建了专门的模型来总结、翻译、搜索和检索。transformer 将所有这些操作统一到一个单一的结构中，能够执行各种各样的任务。\n“采用这个预测下一个单词的简单模型，它……可以做任何事情，”AI 初创公司 Cohere 的首席执行官兼 transformer 论文的合著者 Aidan Gomez 说。\n现在，他们有一种“在整个互联网上训练”的模型，“输出的结果可以完成所有这些工作，并且比以前的任何东西都好”，他说。\n“这是这个故事的神奇之处。”\n","permalink":"https://viitetrix.github.io/blog/posts/%E8%AF%91%E6%96%87-%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9B%A0-transformer-%E8%80%8C%E5%AD%98%E5%9C%A8/","summary":"主要讲述了 Transformer 模型的诞生及其对生成式 AI 发展的关键推动作用，并探讨了其工作原理、应用领域以及潜在的挑战。","title":"[译文] 生成式人工智能因 Transformer 而存在"},{"content":"原文链接：ChatGPT Is a Blurry JPEG of the Web | The New Yorker\n原文发布日期：2023 年 2 月 9 日\nOpenAI 的聊天机器人提供解释，而谷歌提供引用。我们更倾向于哪个？\n2013 年，一家德国建筑公司的员工发现他们的施乐复印机有点古怪：复印的房屋平面图与原件存在细微但重要的差异。原始平面图上，三个房间的面积分别用矩形标为：14.13、21.11 和 17.42 平方米。然而，在复印件中，三个房间的面积都标着 14.13 平方米。该公司联系了计算机科学家 David Kriesel 来调查这个匪夷所思的结果。之所以需要计算机科学家，是因为现代施乐复印机早已不用上世纪六十年代流行的物理静电复印技术，而是先将文件进行数字扫描，再打印出图像文件。再加上几乎所有数字图像文件都会为了节省空间而被压缩，谜底呼之欲出。\n文件压缩分两步：编码，将文件转换成更紧凑的格式；解码，将文件还原。如果还原的文件与原件完全一致，就是无损压缩，信息没有丢失。反之，如果还原的文件只是原件的近似版本，则是有损压缩，部分信息已丢失且无法恢复。无损压缩常用于文本文件和计算机程序，因为在这些领域，哪怕一个字符出错都可能造成灾难性后果。而在可以接受精度损失的情况下，有损压缩则常用于照片、音频和视频。多数时候，图片、歌曲或电影没有被完美还原，我们并不会察觉。只有当文件被极度压缩时，保真度的损失才会变得明显。我们会注意到所谓的压缩失真：比如极度压缩的 JPEG、MPEG 图像的模糊，或低比特率 MP3 听起来会很刺耳。\n施乐复印机使用一种叫 JBIG2 的有损压缩格式，主要用于黑白图像。为了节省空间，复印机会识别出图像中相似的区域，并只保存一份副本；解压时，它会重复使用该副本以重建图像。结果发现，复印机判定标注房间面积的数字足够相似，只需存储其中一个 “ 14.13 ” 并在打印平面图时重复使用。\n施乐复印机使用有损而非无损压缩格式，这本身不是问题。问题在于复印机悄无声息地降低了图像质量，且其压缩失真不易被察觉。如果复印机打印出的只是模糊的文本，大家自然会明白它并非原件的精确复制。而问题恰恰在于，复印机打印的数字清晰可读但却有误，这让复印件看似精确，实则不然。（2014 年，施乐发布了一个补丁来修复此问题。）\n我认为，在讨论 OpenAI 的 ChatGPT 及其他类似程序（AI 研究人员称之为大语言模型）时，有必要牢记施乐复印机的这段往事。复印机和大语言模型之间的相似性或许并不显而易见，但请思考以下场景：假设你即将永远无法访问互联网，为此，你计划将网络上的所有文本压缩保存到私有服务器。不幸的是，你的服务器只有所需空间的百分之一，如果想存储所有内容，就不能使用无损压缩算法。于是你编写了一个有损算法，该算法能识别文本中的统计规律，并将其存储为一种特殊的文件格式。由于你拥有几乎无限的计算能力，你的算法可以识别出极其细微的统计规律，从而实现 100:1 的惊人压缩比。\n现在，失去互联网访问权限似乎也不是那么可怕了，因为你已经在服务器上存储了网络上的所有信息。唯一的问题是，由于文本被高度压缩，你不能通过精确搜索来查找信息，因为你永远无法得到精确匹配，毕竟存储的并非字词本身。为了解决这个问题，你创建了一个界面，它接收提问形式的查询，并给出概括服务器上信息要旨的回答。\n我所描述的听起来很像 ChatGPT，或者说很像任何其他大语言模型。不妨把 ChatGPT 看作是网络上所有文本的一张模糊的 JPEG 图像。它保留了网络上的大部分信息，正如 JPEG 图像保留了高分辨率图像的大部分信息，但是如果你寻找的是一个精确的比特序列，你是找不到的；你得到的永远只是一个近似值。不过，由于这个近似值以语法文本的形式呈现，而这正是 ChatGPT 所擅长的，因此它通常可以接受。你看到的仍然是一张模糊的 JPEG 图像，但其模糊的方式并不会使整个图片看起来失真。\n这种类比不仅有助于理解 ChatGPT 如何通过变换措辞来重新组织网络信息，还可以解释大语言模型（如 ChatGPT）为何容易产生“幻觉”，即给出毫无意义的事实性回答。这些幻觉是压缩失真，但就像施乐复印机产生的错误标签一样，它们看似合理，只有与原件（即网络信息或我们的已有知识）比对后才能识别。这么一想，出现幻觉也就不足为奇了；如果一个压缩算法在丢弃了 99% 的原始内容后还能重建文本，那么其生成的很多内容完全是虚构的，也就在意料之中了。\n当我们意识到有损压缩算法常用的技术是“插值”时，这种类比就更说得通了——插值，就是通过查看空白两侧的内容来推测缺失的信息。当图像程序显示照片，需要重建压缩时丢失的像素时，它会查看附近的像素并计算平均值。这正是 ChatGPT 在收到指令时所做的，例如让它用《独立宣言》的风格描述在烘干机里丢了一只袜子，它会在“词汇空间”中取两个点，并生成填补其间空白的文本（“当人类历史的进程中，一个人有必要将其衣物与其配偶分离，以维护其清洁和秩序……”）。ChatGPT 极擅长这种插值，以至于人们觉得很有趣：这就像给文字而不是图片用了“模糊”工具，而且玩得不亦乐乎。\n鉴于 ChatGPT 等大语言模型常被吹捧为人工智能的前沿技术，将其比作有损文本压缩算法，听起来似乎是轻视，至少令人感到泄气。我承认这种观点有助于纠正人们将大语言模型拟人化的倾向，但这种压缩类比还有另一方面值得思考。自 2006 年起，一位名叫 Marcus Hutter 的 AI 研究员设立了一项奖金——“人类知识压缩奖”，又称“Hutter 奖”——奖励给那些能将维基百科一个特定的一千兆字节大小的快照进行无损压缩、且压缩后文件大小小于之前获奖者的人。你可能遇到过用 zip 格式压缩的文件。zip 格式能将 Hutter 设定的一千兆字节文件压缩到约三百兆字节，而最近的获奖者将其压缩到了 115 兆字节。这不仅仅是一次压缩练习。Hutter 认为，更好的文本压缩将有助于创造人类水平的人工智能，部分原因在于理解文本才能实现最大程度的压缩。\n为了说明压缩和理解之间的关系，想象你有一个文本文件，其中有一百万个加减乘除的例子。尽管任何压缩算法都能减小这个文件的大小，但要实现最大压缩比，可能需要推导出算术原理，然后编写一个计算器程序代码。使用计算器，你不仅可以完美重建这一百万个例子，还可以重建你未来可能遇到的任何其他算术题。同样的逻辑也适用于压缩维基百科的一个切片。如果压缩程序知道力等于质量乘以加速度，它就可以在压缩物理学页面时丢弃很多词语，因为它可以重建这些词语。同样，程序对供求关系了解得越多，它在压缩经济学页面时就能丢弃越多词语，以此类推。\n大语言模型识别文本中的统计规律。对网络文本的任何分析都会表明，“供应短缺”之类的短语经常出现在“价格上涨”之类的短语附近。一个结合了这种相关性的聊天机器人，在被问及供应短缺的影响时，可能会回答价格上涨。如果一个大语言模型汇集了大量经济学术语之间的相关性——多到足以对各种问题给出合理的回答——我们是否能说它真的理解了经济学理论？由于种种原因，像 ChatGPT 这样的模型没有资格获得 Hutter 奖，原因之一是它们不能精确重建原文，也就是说，它们并非无损压缩。但是，难道它们的有损压缩就没有展现出 AI 研究人员感兴趣的那种真正的理解吗？\n让我们回到算术的例子。如果你让 GPT-3（ChatGPT 基于的大语言模型）做两个数的加减法，当数字只有两位数时，它几乎总能给出正确答案。但随着数字变大，它的准确率会显著下降，到五位数时降至 10%。GPT-3 给出的大多数正确答案在网络上都找不到——例如，没有多少网页包含“245 + 821”——所以它并非简单记忆。但是，尽管消化了大量信息，它也未能推导出算术原理。仔细检查 GPT-3 的错误答案会发现，它在做算术时并没有进位。网络上当然有关于进位的解释，但 GPT-3 无法将这些解释纳入其中。GPT-3 对算术例子的统计分析使它能够生成真实算术的粗浅近似，但也仅此而已。\n既然 GPT-3 在小学水平的科目上都会失败，那我们该如何解释它有时在撰写大学水平论文时表现出色呢？尽管大语言模型经常产生幻觉，但在“清醒”时，它们似乎真的理解经济学理论等学科。也许算术是个特例，大语言模型并不擅长。那么在加减法之外的领域，文本中的统计规律是否真的对应着对现实世界的真实认知呢？\n我认为有一个更简单的解释。想象一下，如果 ChatGPT 是一种无损算法，它会是什么样子？如果是这样，它总是会从相关网页上逐字引用来回答问题。我们大概只会把它当作传统搜索引擎的略微改进版，而不会对它如此印象深刻。ChatGPT 改述而非逐字引用网络内容，这使它看起来像一个学生在用自己的话表达思想，而不是简单复述读过的内容；这造成了 ChatGPT 理解材料的假象。对人类学生来说，死记硬背并非真正学习的标志，所以 ChatGPT 不能精确引用网络内容，恰恰让我们以为它学到了什么。对于词语序列，有损压缩看起来比无损压缩更智能。\n人们已经提出了大语言模型的许多用途。将它们看作模糊的 JPEG 图像，可以帮助我们评估它们适合做什么，不适合做什么。我们不妨考虑几种场景。\n大语言模型能否取代传统搜索引擎？要让我们对它们有信心，就需要知道它们没有被灌输政治宣传和阴谋论——我们需要知道 JPEG 图像截取的是网络上的正确部分。但是，即使一个大语言模型只包含我们想要的信息，仍然存在模糊性的问题。有一种模糊性是可以接受的，那就是用不同的词语重新表述信息。还有一种模糊性是彻头彻尾的捏造，当我们需要事实时，这是不可接受的。目前尚不清楚在技术上能否做到在消除不可接受的模糊性的同时，保留可接受的模糊性，但我预计我们很快就会找到答案。\n即便可以限制大语言模型进行捏造，我们是否该用它们来生成网络内容？只有当我们的目标是重新包装网络上已有的信息时，这才说得通。有些公司专门干这个——我们通常称它们为“内容农场”。也许大语言模型的模糊性对它们有用，可以作为避免侵犯版权的一种方式。但总的来说，我认为任何对内容农场有利的事情，对搜索信息的人都是不利的。这种“信息再加工”的兴起，使得我们更难在网上找到所需内容；大语言模型生成的文本在网上发布得越多，网络本身就变得越模糊。\n关于 OpenAI 即将推出的 ChatGPT 继任者 GPT-4 的信息很少。但我将做一个预测：在收集用于训练 GPT-4 的大量文本时，OpenAI 的人会尽力排除 ChatGPT 或任何其他大语言模型生成的材料。如果真是这样，这将无意中证实大语言模型和有损压缩之间的类比是有用的。重复保存 JPEG 会产生更多的压缩失真，因为每次都会丢失更多信息。这相当于过去反复复印的数字版本，图像质量只会越来越差。\n实际上，衡量大语言模型质量的一个有用标准可能是，公司是否愿意使用其生成的文本作为新模型的训练材料。如果 ChatGPT 的输出对 GPT-4 来说不够好，我们就可以认为它对我们来说也不够好。反之，如果一个模型开始生成非常好的文本，好到可以用来训练新模型，那么这应该让我们对该文本的质量充满信心。（我怀疑这样的结果需要在构建这些模型的技术上取得重大突破。）而一旦模型生成的输出质量与其输入不相上下时，有损压缩的比喻就不再适用了。\n大语言模型能否帮助人们创作原创作品？要回答这个问题，我们需要明确这个问题的含义。有一种艺术流派被称为施乐艺术，或复印艺术，其中艺术家将复印机的独特属性用作创作工具。类似于 ChatGPT 的复印机肯定可以做到这一点，所以，从这个意义上说，答案是肯定的。但我认为没有人会声称复印机已成为艺术创作中必不可少的工具；绝大多数艺术家在他们的创作过程中不使用它们，也没有人争辩说他们在做出这种选择时会让自己处于不利地位。\n因此，让我们假设我们不是在谈论一种类似于施乐艺术的新型写作流派。鉴于这一规定，大语言模型生成的文本能否成为作家在创作原创作品（无论是小说还是非小说）时可以借鉴的有益起点呢？让大语言模型处理样板文件是否能让作家将注意力集中在真正有创意的部分？\n显然，没有人可以代表所有作家说话，但让我来论证一下，从一份模糊且非原创的文本开始，并非创作原创作品的良方。如果你是一位作家，在你写出原创作品之前，你会写很多非原创的作品。花费在那些非原创作品上的时间和精力并没有白费；相反，我认为，恰恰是这些努力让你最终得以创作出原创作品。遣词造句、调整语序，这些练习教会了你如何运用文字传情达意。让学生写论文不仅仅是一种测试他们对材料掌握程度的方法；它还为他们提供了表达自己想法的经验。如果学生们写的都是千篇一律的文章，他们就永远无法掌握创作新颖内容所需的技能。\n而且，一旦你不再是一名学生，你就可以安全地使用大语言模型提供的模板，情况并非如此。表达自己想法的挣扎并不会在你毕业后消失——它可能在你每次开始起草新作品时发生。有时，只有在写作的过程中，你才能发现自己的原创想法。有人可能会说，大语言模型的输出看起来与人类作家的初稿并没有什么不同，但是，我认为这是一种肤浅的相似性。你的初稿并非清晰表达的非原创内容，而是一个尚未成熟的原创想法，伴随着你隐约的不满，因为你意识到它与你理想中的表达之间的差距。这就是在重写过程中指导你的东西，也是你在开始使用人工智能生成的文本时所缺少的东西之一。\n写作没有什么神奇或神秘之处，但它不仅仅是将现有文档放在不可靠的复印机上并按下打印按钮。在未来，我们有可能构建一个人工智能，它能够仅根据自己对世界的经验来写出好的散文。我们实现这一目标的那一天将是重要的——但那一天远远超出了我们的预测范围。与此同时，我们有理由问，拥有一个可以改写 Web 的东西有什么用呢？如果我们永远失去了对互联网的访问权限，并且必须将副本存储在空间有限的私有服务器上，那么像 ChatGPT 这样的大语言模型可能是一个好的解决方案，假设它可以防止捏造。但我们并没有失去对互联网的访问权限。所以，当你仍然拥有原件时，一张模糊的 JPEG 图像到底有多大用处呢？♦\n","permalink":"https://viitetrix.github.io/blog/posts/%E8%AF%91%E6%96%87-chatgpt%E4%BA%92%E8%81%94%E7%BD%91%E7%9A%84%E6%A8%A1%E7%B3%8A%E7%BC%A9%E5%BD%B1/","summary":"ChatGPT 像是互联网文本的一个模糊的 JPEG，它提供信息的转述而不是精确的引用，这种有损压缩虽然在表达上看似更智能，却牺牲了信息的准确性，并可能导致“幻觉”现象，即生成看似合理实则错误的内容。","title":"[译文] ChatGPT：互联网的模糊缩影"}]