[{"content":"原文链接：Generative AI exists because of the transformer\n原文发布日期：2023 年 9 月 12日\n在过去的几年里，我们在构建智能机器这条长达数十年的探索之路上实现了一个巨大的飞跃：大语言模型的出现。\n这项基于人脑模拟研究的技术催生了一个被称为“生成式 AI”的新领域——这种软件能够创作出与人类水平相当的、可信且复杂的文本、图像和计算机代码。\n全球的企业已经开始探索这项新技术的应用，相信它将颠覆媒体、金融、法律和专业服务以及教育等公共服务领域。大语言模型基于被称为 transformer 模型的一项科学突破，它由谷歌研究人员在 2017 年提出。\n“虽然我们一直都清楚我们 transformer 研究的突破性意义，但几年后，我们对其在医疗保健、机器人技术和安全等新领域的持久潜力感到振奋，它增强了人类的创造力等等。”谷歌高级研究员 Slav Petrov 说道，他致力于构建包括大语言模型在内的 AI 智能体。\n大语言模型备受推崇的优势——通过编写和分析文本来提高生产力的能力——也是它对人类构成威胁的原因。根据高盛的说法，它可能使大型经济体中相当于 3 亿名全职员工面临自动化，导致大范围失业。\n随着这项技术迅速融入我们的生活，理解大语言模型如何生成文本意味着理解为什么这些模型是如此多才多艺的认知引擎——以及它们还能帮助创造什么。\n要编写文本，大语言模型必须首先将单词转换成它们能理解的语言。\n首先，一段文字会被分解成 token ——这是可以被编码的基本单元。虽然 token 通常表示单词的一部分，但在本例中，为了简单起见，我们把每个完整的单词都当作一个 token。\n为了理解一个词的含义，以我们例子中的 work 为例，大语言模型首先会在海量的训练数据中观察它的上下文，并注意它的邻近词 。这些数据集基于互联网上发布的文本整理而成，新的大语言模型使用了数十亿个单词进行训练。\n最终，我们会得到一个庞大的单词集合，其中既包括在训练数据中与 work 一同出现的词，也包括那些没有和它一起出现的词。\n当大语言模型 处理 这些单词时，会生成一个向量（也就是一个数值列表），并根据每个单词在训练数据中与 work 的邻近程度来调整这个向量。这个向量被称为词嵌入。\n词嵌入可以包含数百个数值，每个数值代表单词含义的某个不同方面。就像你可以通过房子的特征——类型、位置、卧室数量、浴室数量、楼层数——来描述它一样，词嵌入中的数值量化了一个单词的语言特征。\n由于这些特征的提取方式，我们并不确切知道每个数值代表什么含义，但那些我们预期会以类似方式使用的单词，通常会有相似的词嵌入。\n例如，sea 和 ocean 这两个词可能不会在完全相同的上下文中使用（“all at ocean”不能直接替代“all at sea”，后者表示“不知所措”），但它们的含义非常相近，而词嵌入可以让我们量化这种相近的程度\n通过把每个词嵌入所代表的数百个数值缩减到只有两个，我们就能更清楚地看到这些单词之间的距离。\n我们可能会发现一些 代词 或 交通方式 的集群，而能够以这种方式量化单词，正是大语言模型生成文本的第一步。\n但这并不是大语言模型如此聪明的全部原因。真正让它们能够如此流畅地解析和写作的，是一种名为 transformer 的工具。它从根本上加速并增强了计算机理解语言的方式。\nTransformer 可以一次性处理整个序列 —— 无论是句子、段落还是整篇文章 —— 分析其所有组成部分，而不仅仅是单个词汇。\n这使得软件能够更好地捕捉上下文和模式，并更准确地翻译或生成文本。这种同步处理还使得大语言模型的训练速度更快，从而提高了它们的效率和扩展能力。\n2017 年 6 月，谷歌的八位 AI 研究人员首次发表了概述 transformer 模型的论文。他们这份 11 页的研究论文标志着生成式 AI 时代的开始。\nTransformer 架构的一个关键概念是自注意力机制。这使得 LLM 能够理解单词之间的关系。\n自注意力机制查看一段文本中的每个 token ，并判断哪些 token 对于理解其含义最重要。\n在 transformer 之前，最先进的 AI 翻译方法是循环神经网络 (RNNs)，它会扫描句子中的每个单词并按顺序处理。\n通过自注意力机制，transformer 可以同时计算句子中的所有单词。捕捉这种上下文使大语言模型具备了更复杂的语言解析能力。\n在这个例子中，通过一次性评估整个句子，transformer 能够理解 interest 在这里是用作名词，表示个人对政治的兴趣。\n如果我们调整一下句子\n模型会理解 interest 现在被用作金融意义上的“利息”。\n当我们将两个句子组合在一起时，由于模型对伴随文本的关注，它仍然能够识别每个单词的正确含义。\n在第一种用法中，模型最关注的是 no 和 in 。\n而在第二种用法中，模型最关注的是 rate 和 bank 。\n这个功能对于高级文本生成至关重要。如果没有它，在某些上下文中可以互换但在其他上下文中不能互换的单词可能会被错误地使用。\n简而言之，自注意力确保了在总结关于利率的句子时，不会误用 热情 一词。\n这种能力不仅仅局限于像“interest”这样具有多重含义的单词。\n在下面的句子中，自注意力机制能够计算出 it 最有可能指的是 dog 。\n如果我们改变句子，将 hungry 换成 delicious ，模型能够重新计算， it 现在最有可能指的是 bone 。\n自注意力机制对语言处理的好处随着规模的扩大而增加。它允许 LLM 从句子边界之外获取 上下文 ，使模型更好地理解单词的使用方式和时间。\nGPT-4 是全球最大且最先进的大语言模型之一，是 OpenAI 最新的 AI 模型。该公司称，该模型在美国律师资格考试、大学预修课程（AP）考试和 SAT 学术能力评估测试等多个学术和专业基准测试中展现出“人类水平的表现”。\nGPT-4 可以生成和接收大量文本：用户可以输入多达 25,000 个英文单词，这意味着它可以处理详细的财务文件、文学作品或技术手册。\n该产品的出现重塑了整个科技行业，包括谷歌、Meta 和微软这些支持 OpenAI 的科技巨头，以及一些小型初创公司，都在竞相抢占这一领域的领先地位。\n他们发布的知名大语言模型包括：谷歌的 PaLM 模型（为其聊天机器人 Bard 提供支持）、Anthropic 的 Claude 模型、Meta 的 LLaMA 模型，以及 Cohere 的 Command 模型等。\n虽然这些模型已经被许多企业采用，但其背后的一些公司正面临着围绕其使用从网络上抓取的受版权保护的文本、图像和音频的法律纠纷 。\n其原因在于，当前的大语言模型几乎都是基于整个英语互联网进行训练的——这使得它们比前几代模型强大得多。\n从这个庞大的词语和图像语料库中，这些模型学习如何识别模式，并最终预测下一个最佳词语。\n简单来说，在对提示进行 token 化和编码后，我们就得到一个数据块，它以机器能理解的方式表示我们的输入，包括词语之间的含义、位置和关系。\n该模型现在的目标是预测序列中的下一个单词，并重复执行此操作，直到生成完整的文本输出。\n为此，该模型会为每个 token 分配一个 概率分数，表示其成为序列中下一个单词的可能性。\n它会持续执行此操作，直到生成完整的文本输出。\n但是，这种孤立地预测下一个单词的方法（称为“贪婪搜索”）可能会产生问题。有时，尽管每个单独的 token 可能是下一个最佳匹配，但整个短语的相关性可能会降低。\n不一定总是错误的，但可能也不是你所期望的。\nTransformer 使用多种方法来解决这个问题并提高其输出质量。其中一个例子叫做 集束搜索 。\n它不只关注序列中的下一个单词，而是将更大范围的 token 集合作为一个整体来考虑其概率。\n通过集束搜索，该模型能够考虑多条路径并找到最佳选项。\n这产生了更好的结果，最终生成更连贯、更像人类的文本。\n但是事情并不总按计划进行。虽然文本可能看起来合理且连贯，但它并不总是与事实相符。大语言模型不是查找事实的搜索引擎；它们是模式识别引擎，猜测序列中的下一个最佳选项。\n由于这种固有的预测性质，大语言模型还可以在研究人员称之为“幻觉”的过程中捏造信息。它们可以生成虚构的数字、姓名、日期、引言——甚至是网络链接或整篇文章。\n大语言模型的使用者分享了一些示例。其中包括指向《金融时报》和彭博社不存在的新闻文章的链接，对研究论文的虚假引用，已出版书籍的错误作者，以及充满事实错误的传记。\n在纽约发生的一起引人注目的事件中，一位律师使用 ChatGPT 为一个案件撰写了一份简报。当辩方质疑这份报告时，他们发现其中充斥着虚构的司法意见和法律引文。“我不知道 ChatGPT 会编造案例，”这位律师后来在自己的法庭听证会上告诉法官。\n尽管研究人员表示幻觉永远不会被完全消除，但 Google、OpenAI 和其他公司正在努力通过一个称为“溯源”的过程来限制它们。这包括将大语言模型的输出与网络搜索结果进行交叉检查，并向用户提供引文以便他们进行验证。\n人类也被用来提供反馈和填补信息空白——这个过程被称为基于人类反馈的强化学习 (RLHF)——这进一步提高了输出的质量。但是，了解哪些查询可能会触发这些幻觉，以及如何预测和减少它们，仍然是一个巨大的研究挑战。\n尽管存在这些局限性，transformer 还是催生了大量尖端 AI 应用。除了为 Bard 和 ChatGPT 等聊天机器人提供支持外，它还驱动我们手机键盘上的自动完成和智能扬声器中的语音识别。\n然而，它真正的力量在于语言之外。它的发明者发现，transformer 模型可以识别和预测任何重复的主题或模式。从图像中的像素（使用 Dall-E、Midjourney 和 Stable Diffusion 等工具），到使用 GitHub CoPilot 等生成器的计算机代码。它甚至可以预测音乐中的音符和蛋白质中的 DNA，以帮助设计药物分子。\n几十年来，研究人员构建了专门的模型来总结、翻译、搜索和检索。transformer 将所有这些操作统一到一个单一的结构中，能够执行各种各样的任务。\n“采用这个预测下一个单词的简单模型，它……可以做任何事情，”AI 初创公司 Cohere 的首席执行官兼 transformer 论文的合著者 Aidan Gomez 说。\n现在，他们有一种“在整个互联网上训练”的模型，“输出的结果可以完成所有这些工作，并且比以前的任何东西都好”，他说。\n“这是这个故事的神奇之处。”\n","permalink":"https://viitetrix.github.io/blog/posts/%E8%AF%91%E6%96%87-%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9B%A0-transformer-%E8%80%8C%E5%AD%98%E5%9C%A8/","summary":"主要讲述了 Transformer 模型的诞生及其对生成式 AI 发展的关键推动作用，并探讨了其工作原理、应用领域以及潜在的挑战。","title":"[译文] 生成式人工智能因 Transformer 而存在"},{"content":"译文链接：[译文] ChatGPT：互联网的模糊缩影 | Bit by Bit\n作者信息 Ted Chiang\n中文名：姜峯楠 生于 1967 年，美国科幻小说作家 星云奖、雨果奖获得者 作品： 小说集《你一生的故事》 小说集《呼吸》 短篇小说《你一生的故事》被改编为科幻电影《降临》 Ted Chiang | The New Yorker 投稿 内容简记 JPEG 是一种图像压缩格式，手机里拍的照片、网上看到的图片，很多都是 JPEG 格式的。它的特点是“有损压缩”，也就是说，为了减小文件大小，JPEG 会丢掉一些图像信息。丢掉的信息越多，图片就越模糊，文件也就越小。\nChatGPT 这样的 LLM，其实就是一个有损压缩版的互联网。它把网上的海量文本压缩成了一个模型，这个模型能生成各种看似通顺的文本，但它本质上是对原文的一种“模糊”处理,我们得到的永远只是以语法文本的形式呈现一个近似值。\n它学习的方式，和 JPEG 压缩图片有点像——它记住的不是每一个字、每一句话，而是“统计规律”。就像 JPEG 只记住了图片的“大致轮廓”，ChatGPT 记住的也只是语言的“大致模式”。\n所以，当我们问 ChatGPT 一个问题，它给你的答案，并不是从网上找到原文复制粘贴给的，而是根据它记住的“模式”，重新组织语言，给你一个“看起来差不多”的回答。就像 JPEG 给你的图片，虽然看起来和原图很像，但其实已经丢失了很多细节。\nLLM 的幻觉(hallucinations) 问题就像有损压缩的“副作用”。因为模型在“压缩”信息的时候，会根据一些统计规律来“脑补”缺失的部分，就像图像软件会根据周围的像素来“猜”一个丢失的像素一样。ChatGPT 有时候会一本正经地胡说八道，生成一些看似有道理、实则驴唇不对马嘴的文本。\nChatGPT 的算术能力离谱的差，更加证明了它并没有真正理解算术的原理，它知道进位这个概念却不知道如何运用，只是依靠记住的大量的例子之间的统计规律来给出答案。而我们学算术是学“规则”，而不是死记硬背“例子”。\nChatGPT 的“模糊”有时候看起来反而更“智能”，如果我们让 ChatGPT 直接引用原文，它看起来就像一个搜索引擎，我们反而会觉得它不够智能。而当它用自己的话把网上的信息重新“包装”一下，它就更像一个理解了内容的学生，而不是一个只会“死记硬背”的书呆子。\n“如果还有原图，那要这张模糊的 JPEG 干嘛？” ChatGPT 这类工具确实很强大，但它们也只是工具。ChatGPT 这类工具确实很强大，但它们也只是工具。真正重要的，还是我们人类的独立思考和创造力。\n拓展 JPEG 有损压缩只能作为辅助理解 LLM 某些特性的类比，而不能作为其工作原理的解释 JPEG 有损压缩类比的优点与缺点 优点： LLM 生成的文本，是对原始文本的一种“抽象”和“概括”，就像 JPEG 图片是对原始图片的一种“压缩”和“模糊”。 LLM 可能会“脑补”一些信息，就像 JPEG 图片可能会出现一些“失真”。 缺点： 无法解释 LLM 的核心机制 LLM 是如何学习语言的 LLM 是如何理解上下文的 LLM 是如何生成连贯的文本的 “完形填空”或许可以作为浅显理解大语言模型工作原理更贴切的类比 学习过程的相似性 先想想我们当年是怎么刷题的？是不是一本接一本的《五年高考三年模拟》，埋头苦干，就为了掌握那些出题的“套路”？LLM 也差不多，只不过它“刷”的是海量的文本数据，从里面学习语言的规律。如果把海量数据比作题海，那么可以说，AI的学习也是题海战术，而且不眠不休。\n核心机制的相似性 做完形填空的时候，我们是不是要根据上下文和选项，猜那个空里应该填啥？LLM 也一样，只不过它的“词库”大得惊人，每个词都有一个出现的概率。这就好比学霸做题都是选出最优解，而学渣做题时一半靠蒙，会想到“嗯，这个空填 A 的概率好像比 B 高一点”。这就有点像好模型和差模型了\nLLM 生成文本，就是一个词一个词（或者一个 token 一个 token）地“吐”出来，连起来就成了一句完整的话。这就更像是一个一个空格出现在句尾的完形填空。\n上下文理解的依赖性 做完形填空的时候，得联系上下文，才能选出最合适的那个词。如果上下文信息很少，或者很模糊，你就很难选对。LLM 也是，你给它的“提示”（prompt）越清楚，它就越能理解你的意思，生成的内容也越准确。这就像你跟暧昧对象聊天，你们之间的“背景信息”越多，就越能秒懂对方的意思。如果你突然来一句“在吗？”，估计对方心里会想：“这人又要干嘛？”\n双向信息的利用 做完形填空，不仅要看前面的句子，还要考虑后面的内容，这样才能填得更准确。 LLM 也学会了这一招，它用了一种叫做“Transformer”的架构，能够同时关注上下文的信息，这就是所谓的“注意力机制”。 这就像你在聚会上，不仅要听别人说了什么，还要观察周围人的反应，才能更好地接话。\n输出结果/质量的可比性： “完形填空”和 LLM 都在追求让一句话、一段话，甚至一篇文章，读起来“通顺”和“合理”。\nLLM 一本正经的胡说八道到底是怎么发生的 LLM 的“幻觉”，简单来说，就是 AI 一本正经地胡说八道，生成一些看似合理、实则完全不正确或与现实不符的内容。\n数据偏差：垃圾进，垃圾出 训练 LLM 需要喂给它海量的数据，就像养孩子一样，你给它吃什么，它就长成什么样。如果这些数据本身就存在偏差，比如充斥着各种谣言、偏见或者错误信息，亦或是不同来源的数据互相矛盾，LLM 是无法判断哪些数据有问题的，自然就无法进行可靠生成。\n有时候数据即使是真实的，但是数据分布不平衡也会带来问题。比如：网上关于“猫咪从高处落下也能毫发无损”的信息，远比“猫咪从高处落下会受伤”要多，LLM读多了，可能也会产生“猫咪怎么摔都不会受伤”的幻觉。热门领域信息丰富，冷门领域数据稀疏，也会造成认知偏差。所以训练数据的配比也非常重要。\n同时由于 LLM 的训练数据都有截至日期，在此之后的数据 LLM 并没有获取，它也不会自己实时更新，就会导致数据过时，与现在的认知产生冲突。\n上下文局限：能看到的只有这么多 LLM 对于文本的理解是有上下文窗口限制的，如果是长对话，那么早期提到的重要信息很可能在后续的回答中已经被“遗忘”了。这就像左耳朵进右耳朵出，只有中间脑子那么一小段距离的信息存在 LLM 的记忆中。\n抽象能力的缺失：只能模仿，无法“举一反三” 人类的学习，不仅仅是记住信息，更重要的是理解信息背后的逻辑和规律，并进行抽象和概括，将知识应用于不同的领域。但 LLM 目前还不具备这种能力。 它就像一个只会死记硬背的书呆子，虽然记住了很多公式和定理，但考试的时候还是不会做题。\n同时 LLM 并不懂得将其所接收的大量零散的知识打造成系统化的知识体系，知识还在，但失去了整体性，就会产生偏差。\n模式匹配的陷阱：看似聪明，实则机械 LLM 本质上是一个“模式匹配”机器，它擅长找出数据中的各种模式，然后根据这些模式来生成新的内容。但这也会导致一个问题：它可能会过度解读数据中的一些巧合或者虚假关联，从而产生一些不切实际的“幻觉”。\n比如 LLM 并不能发现数据模式之间到底是因果关系还是巧合.好比每次下雨前，蚂蚁都会搬家，于是就得出结论：蚂蚁搬家导致了下雨。这显然是荒谬的。\n缺乏实践验证：只会纸上谈兵 LLM 的知识来源于文本而不是实践经验，它没有办法通过实验来验证所获取的各种建设，也没办法对真实物理世界产生反馈，那么就不可能像人类一样从错误中学习和纠正。\n\u0026ldquo;涌现\u0026rdquo;：叛逆少年的出现 LLM 在训练过程中，随着参数、数据量等规模的扩大，会自发产生一些非设定的未明确训练的能力或者行为，这就是“涌现”。它也代表着一些不确定性。 想想一个孩子成长到了叛逆期，会做出一些只符合自己想法父母预料不到的事情。而 LLM 的涌现能力同样也无法预测，可能会促使模型自我“脑补”生成一些新颖但是未经证实的内容，或者在缺乏足够信息的时候却“过度自信”。\n概率游戏：没有事实，只有概率 LLM 给出答案，不是因为它“知道”什么是对什么是错，而是因为它根据自己学到的东西，计算出每个词出现的概率，然后进行选择。\n常在河边走，哪有不湿鞋。从概率的角度看，只要存在不确定性，就有可能出错。\n而只要一步出错，就会出现多米诺骨牌效应，一步错步步错，造成后续的结果整段偏离。\nLLM 为什么算术都做不好 LLM 的设计目标本来就不是做算术 LLM 被训练出来是为了理解和生成自然语言，而不是做算术运算。算术本来就不是 LLM 的强项。让 LLM 去做算术运算无异于让一个哲学家去解决数学问题，专业不对口啊。\n当然这只是次要因素。\nLLM 的工作机制是序列处理 LLM 在一般情况下对于数字的处理和文本一样，是按照位置顺序一个一个来的，比如说“123 + 456”，LLM 的处理视角是 “1” “2” “3” “+” “4” “5” “6” ，序列处理的局限性决定了 LLM 不能像人类一样整体把握数字，而将数字进行拆分本来就失去了其代表的含义，更不用说保证数字的精度了。\nLLM 的本质是概率生成 LLM 生成答案是基于概率分布的，即便进行算术运算也无法进行确定性生成，也就是说对于答案的“5” “7” “9”，LLM 每一步都是在掷骰子，每一步都可能产生误差。对于简单的数字可能有训练集囊括等 buff 加成出现大概率正确，但随着数字的增大，buff 加成消失，这种不确定性的累计必然会导致最终结果的偏差。LLM 能记住的，只是那些在训练数据中出现过的“特定”算式。\n数学问题训练数据的特点 有人可能会说，既然 LLM 将所有数学问题全部囊括在训练集中，增加 buff 可不可行？\n答案肯定是不可能。\n首先在正常互联网文本数据中，数学问题的样本本来就相对较少，尤其是复杂的数学运算出现的频率更低，这自然决定了占据小比例的数学问题无法成为 LLM 的专项。\n而增大数学问题样本比例，首当其中的就会影响训练数据配比，导致 LLM 在自然语言处理的主线任务上出现差错。更何况同一个数学问题在训练数据同可能有多种表达方式，单单一个算术就有无限数量的数据可以表现，将其全部囊括进样本是不可能实现的。\n抽象思维才是人类的王牌 既然囊括不了，那么教会 LLM 数学运算原理不行就了？这同样实现不了。\n我们知道关于数学运算的原理其实存在于训练集中，问题是 LLM 只会死记硬背，不会活学活用啊。\nLLM 对于抽象推理思维的缺乏决定了它虽然记住了原理，但是它理解不了运算背后的逻辑关系，更不要提利用原理推导了。单单数学概念中简单的进位、错位的概念都够 LLM 喝一壶的。\n所以，算术运算，计算器是比 LLM 更合适的选择，当然让 LLM 调取计算器等工具辅助就更合适不过了。\n","permalink":"https://viitetrix.github.io/blog/posts/%E7%AC%94%E8%AE%B0-chatgpt%E4%BA%92%E8%81%94%E7%BD%91%E7%9A%84%E6%A8%A1%E7%B3%8A%E7%BC%A9%E5%BD%B1/","summary":"JPEG 有损压缩只能作为辅助理解 LLM 某些特性的类比，而不能作为其工作原理的解释；LLM 一本正经的胡说八道到底是怎么发生的；LLM 为什么算术都做不好","title":"[笔记] ChatGPT：互联网的模糊缩影"},{"content":"看到有人说 AI 辅助阅读导致阅读能力下降，被大纲以及思维导图生成这种榨汁读法带到沟里去了。\n其实这其中最主要的问题是没有把握这个度，选择了放弃主动思考，认为人阅读的意义就是获取信息， AI 榨出来的汁就是营养。但想想看，相比较自己直接吃果肉来说，是不是缺失了很多东西？信息和知识能一样吗？\n更何况，即便没有 AI，也有各种小明小美演绎着 10 分钟的电影速攻，这是原本的电影吗？\n尤其是在现在这种信息爆炸的时代，唾手可得的永远不是真正的答案，透过 AI 这个望远镜我们看到的是探索前方的缩略，而非全貌。\n真正想要将知识内化成为能力，只有亲身去经历、去思考、去实践。毕竟，有些路我们必须亲自走过，AI 只是书童，进京赶考的永远是我们自己。\n","permalink":"https://viitetrix.github.io/blog/posts/%E6%80%9D%E8%80%83-%E4%B9%A6%E7%AB%A5%E8%99%BD%E5%A5%BD%E5%B0%B1%E6%98%AF%E4%B8%8D%E8%83%BD%E6%9B%BF%E4%BD%A0%E8%BF%9B%E4%BA%AC%E8%B5%B6%E8%80%83/","summary":"\u003cp\u003e看到有人说 AI 辅助阅读导致阅读能力下降，被大纲以及思维导图生成这种榨汁读法带到沟里去了。\u003c/p\u003e\n\u003cp\u003e其实这其中最主要的问题是没有把握这个度，选择了放弃主动思考，认为人阅读的意义就是获取信息， AI 榨出来的汁就是营养。但想想看，相比较自己直接吃果肉来说，是不是缺失了很多东西？信息和知识能一样吗？\u003c/p\u003e","title":"[思考] 书童虽好，就是不能替你进京赶考"},{"content":"原文链接：ChatGPT Is a Blurry JPEG of the Web | The New Yorker\n原文发布日期：2023 年 2 月 9 日\nOpenAI 的聊天机器人提供解释，而谷歌提供引用。我们更倾向于哪个？\n2013 年，一家德国建筑公司的员工发现他们的施乐复印机有点古怪：复印的房屋平面图与原件存在细微但重要的差异。原始平面图上，三个房间的面积分别用矩形标为：14.13、21.11 和 17.42 平方米。然而，在复印件中，三个房间的面积都标着 14.13 平方米。该公司联系了计算机科学家 David Kriesel 来调查这个匪夷所思的结果。之所以需要计算机科学家，是因为现代施乐复印机早已不用上世纪六十年代流行的物理静电复印技术，而是先将文件进行数字扫描，再打印出图像文件。再加上几乎所有数字图像文件都会为了节省空间而被压缩，谜底呼之欲出。\n文件压缩分两步：编码，将文件转换成更紧凑的格式；解码，将文件还原。如果还原的文件与原件完全一致，就是无损压缩，信息没有丢失。反之，如果还原的文件只是原件的近似版本，则是有损压缩，部分信息已丢失且无法恢复。无损压缩常用于文本文件和计算机程序，因为在这些领域，哪怕一个字符出错都可能造成灾难性后果。而在可以接受精度损失的情况下，有损压缩则常用于照片、音频和视频。多数时候，图片、歌曲或电影没有被完美还原，我们并不会察觉。只有当文件被极度压缩时，保真度的损失才会变得明显。我们会注意到所谓的压缩失真：比如极度压缩的 JPEG、MPEG 图像的模糊，或低比特率 MP3 听起来会很刺耳。\n施乐复印机使用一种叫 JBIG2 的有损压缩格式，主要用于黑白图像。为了节省空间，复印机会识别出图像中相似的区域，并只保存一份副本；解压时，它会重复使用该副本以重建图像。结果发现，复印机判定标注房间面积的数字足够相似，只需存储其中一个 “ 14.13 ” 并在打印平面图时重复使用。\n施乐复印机使用有损而非无损压缩格式，这本身不是问题。问题在于复印机悄无声息地降低了图像质量，且其压缩失真不易被察觉。如果复印机打印出的只是模糊的文本，大家自然会明白它并非原件的精确复制。而问题恰恰在于，复印机打印的数字清晰可读但却有误，这让复印件看似精确，实则不然。（2014 年，施乐发布了一个补丁来修复此问题。）\n我认为，在讨论 OpenAI 的 ChatGPT 及其他类似程序（AI 研究人员称之为大语言模型）时，有必要牢记施乐复印机的这段往事。复印机和大语言模型之间的相似性或许并不显而易见，但请思考以下场景：假设你即将永远无法访问互联网，为此，你计划将网络上的所有文本压缩保存到私有服务器。不幸的是，你的服务器只有所需空间的百分之一，如果想存储所有内容，就不能使用无损压缩算法。于是你编写了一个有损算法，该算法能识别文本中的统计规律，并将其存储为一种特殊的文件格式。由于你拥有几乎无限的计算能力，你的算法可以识别出极其细微的统计规律，从而实现 100:1 的惊人压缩比。\n现在，失去互联网访问权限似乎也不是那么可怕了，因为你已经在服务器上存储了网络上的所有信息。唯一的问题是，由于文本被高度压缩，你不能通过精确搜索来查找信息，因为你永远无法得到精确匹配，毕竟存储的并非字词本身。为了解决这个问题，你创建了一个界面，它接收提问形式的查询，并给出概括服务器上信息要旨的回答。\n我所描述的听起来很像 ChatGPT，或者说很像任何其他大语言模型。不妨把 ChatGPT 看作是网络上所有文本的一张模糊的 JPEG 图像。它保留了网络上的大部分信息，正如 JPEG 图像保留了高分辨率图像的大部分信息，但是如果你寻找的是一个精确的比特序列，你是找不到的；你得到的永远只是一个近似值。不过，由于这个近似值以语法文本的形式呈现，而这正是 ChatGPT 所擅长的，因此它通常可以接受。你看到的仍然是一张模糊的 JPEG 图像，但其模糊的方式并不会使整个图片看起来失真。\n这种类比不仅有助于理解 ChatGPT 如何通过变换措辞来重新组织网络信息，还可以解释大语言模型（如 ChatGPT）为何容易产生“幻觉”，即给出毫无意义的事实性回答。这些幻觉是压缩失真，但就像施乐复印机产生的错误标签一样，它们看似合理，只有与原件（即网络信息或我们的已有知识）比对后才能识别。这么一想，出现幻觉也就不足为奇了；如果一个压缩算法在丢弃了 99% 的原始内容后还能重建文本，那么其生成的很多内容完全是虚构的，也就在意料之中了。\n当我们意识到有损压缩算法常用的技术是“插值”时，这种类比就更说得通了——插值，就是通过查看空白两侧的内容来推测缺失的信息。当图像程序显示照片，需要重建压缩时丢失的像素时，它会查看附近的像素并计算平均值。这正是 ChatGPT 在收到指令时所做的，例如让它用《独立宣言》的风格描述在烘干机里丢了一只袜子，它会在“词汇空间”中取两个点，并生成填补其间空白的文本（“当人类历史的进程中，一个人有必要将其衣物与其配偶分离，以维护其清洁和秩序……”）。ChatGPT 极擅长这种插值，以至于人们觉得很有趣：这就像给文字而不是图片用了“模糊”工具，而且玩得不亦乐乎。\n鉴于 ChatGPT 等大语言模型常被吹捧为人工智能的前沿技术，将其比作有损文本压缩算法，听起来似乎是轻视，至少令人感到泄气。我承认这种观点有助于纠正人们将大语言模型拟人化的倾向，但这种压缩类比还有另一方面值得思考。自 2006 年起，一位名叫 Marcus Hutter 的 AI 研究员设立了一项奖金——“人类知识压缩奖”，又称“Hutter 奖”——奖励给那些能将维基百科一个特定的一千兆字节大小的快照进行无损压缩、且压缩后文件大小小于之前获奖者的人。你可能遇到过用 zip 格式压缩的文件。zip 格式能将 Hutter 设定的一千兆字节文件压缩到约三百兆字节，而最近的获奖者将其压缩到了 115 兆字节。这不仅仅是一次压缩练习。Hutter 认为，更好的文本压缩将有助于创造人类水平的人工智能，部分原因在于理解文本才能实现最大程度的压缩。\n为了说明压缩和理解之间的关系，想象你有一个文本文件，其中有一百万个加减乘除的例子。尽管任何压缩算法都能减小这个文件的大小，但要实现最大压缩比，可能需要推导出算术原理，然后编写一个计算器程序代码。使用计算器，你不仅可以完美重建这一百万个例子，还可以重建你未来可能遇到的任何其他算术题。同样的逻辑也适用于压缩维基百科的一个切片。如果压缩程序知道力等于质量乘以加速度，它就可以在压缩物理学页面时丢弃很多词语，因为它可以重建这些词语。同样，程序对供求关系了解得越多，它在压缩经济学页面时就能丢弃越多词语，以此类推。\n大语言模型识别文本中的统计规律。对网络文本的任何分析都会表明，“供应短缺”之类的短语经常出现在“价格上涨”之类的短语附近。一个结合了这种相关性的聊天机器人，在被问及供应短缺的影响时，可能会回答价格上涨。如果一个大语言模型汇集了大量经济学术语之间的相关性——多到足以对各种问题给出合理的回答——我们是否能说它真的理解了经济学理论？由于种种原因，像 ChatGPT 这样的模型没有资格获得 Hutter 奖，原因之一是它们不能精确重建原文，也就是说，它们并非无损压缩。但是，难道它们的有损压缩就没有展现出 AI 研究人员感兴趣的那种真正的理解吗？\n让我们回到算术的例子。如果你让 GPT-3（ChatGPT 基于的大语言模型）做两个数的加减法，当数字只有两位数时，它几乎总能给出正确答案。但随着数字变大，它的准确率会显著下降，到五位数时降至 10%。GPT-3 给出的大多数正确答案在网络上都找不到——例如，没有多少网页包含“245 + 821”——所以它并非简单记忆。但是，尽管消化了大量信息，它也未能推导出算术原理。仔细检查 GPT-3 的错误答案会发现，它在做算术时并没有进位。网络上当然有关于进位的解释，但 GPT-3 无法将这些解释纳入其中。GPT-3 对算术例子的统计分析使它能够生成真实算术的粗浅近似，但也仅此而已。\n既然 GPT-3 在小学水平的科目上都会失败，那我们该如何解释它有时在撰写大学水平论文时表现出色呢？尽管大语言模型经常产生幻觉，但在“清醒”时，它们似乎真的理解经济学理论等学科。也许算术是个特例，大语言模型并不擅长。那么在加减法之外的领域，文本中的统计规律是否真的对应着对现实世界的真实认知呢？\n我认为有一个更简单的解释。想象一下，如果 ChatGPT 是一种无损算法，它会是什么样子？如果是这样，它总是会从相关网页上逐字引用来回答问题。我们大概只会把它当作传统搜索引擎的略微改进版，而不会对它如此印象深刻。ChatGPT 改述而非逐字引用网络内容，这使它看起来像一个学生在用自己的话表达思想，而不是简单复述读过的内容；这造成了 ChatGPT 理解材料的假象。对人类学生来说，死记硬背并非真正学习的标志，所以 ChatGPT 不能精确引用网络内容，恰恰让我们以为它学到了什么。对于词语序列，有损压缩看起来比无损压缩更智能。\n人们已经提出了大语言模型的许多用途。将它们看作模糊的 JPEG 图像，可以帮助我们评估它们适合做什么，不适合做什么。我们不妨考虑几种场景。\n大语言模型能否取代传统搜索引擎？要让我们对它们有信心，就需要知道它们没有被灌输政治宣传和阴谋论——我们需要知道 JPEG 图像截取的是网络上的正确部分。但是，即使一个大语言模型只包含我们想要的信息，仍然存在模糊性的问题。有一种模糊性是可以接受的，那就是用不同的词语重新表述信息。还有一种模糊性是彻头彻尾的捏造，当我们需要事实时，这是不可接受的。目前尚不清楚在技术上能否做到在消除不可接受的模糊性的同时，保留可接受的模糊性，但我预计我们很快就会找到答案。\n即便可以限制大语言模型进行捏造，我们是否该用它们来生成网络内容？只有当我们的目标是重新包装网络上已有的信息时，这才说得通。有些公司专门干这个——我们通常称它们为“内容农场”。也许大语言模型的模糊性对它们有用，可以作为避免侵犯版权的一种方式。但总的来说，我认为任何对内容农场有利的事情，对搜索信息的人都是不利的。这种“信息再加工”的兴起，使得我们更难在网上找到所需内容；大语言模型生成的文本在网上发布得越多，网络本身就变得越模糊。\n关于 OpenAI 即将推出的 ChatGPT 继任者 GPT-4 的信息很少。但我将做一个预测：在收集用于训练 GPT-4 的大量文本时，OpenAI 的人会尽力排除 ChatGPT 或任何其他大语言模型生成的材料。如果真是这样，这将无意中证实大语言模型和有损压缩之间的类比是有用的。重复保存 JPEG 会产生更多的压缩失真，因为每次都会丢失更多信息。这相当于过去反复复印的数字版本，图像质量只会越来越差。\n实际上，衡量大语言模型质量的一个有用标准可能是，公司是否愿意使用其生成的文本作为新模型的训练材料。如果 ChatGPT 的输出对 GPT-4 来说不够好，我们就可以认为它对我们来说也不够好。反之，如果一个模型开始生成非常好的文本，好到可以用来训练新模型，那么这应该让我们对该文本的质量充满信心。（我怀疑这样的结果需要在构建这些模型的技术上取得重大突破。）而一旦模型生成的输出质量与其输入不相上下时，有损压缩的比喻就不再适用了。\n大语言模型能否帮助人们创作原创作品？要回答这个问题，我们需要明确这个问题的含义。有一种艺术流派被称为施乐艺术，或复印艺术，其中艺术家将复印机的独特属性用作创作工具。类似于 ChatGPT 的复印机肯定可以做到这一点，所以，从这个意义上说，答案是肯定的。但我认为没有人会声称复印机已成为艺术创作中必不可少的工具；绝大多数艺术家在他们的创作过程中不使用它们，也没有人争辩说他们在做出这种选择时会让自己处于不利地位。\n因此，让我们假设我们不是在谈论一种类似于施乐艺术的新型写作流派。鉴于这一规定，大语言模型生成的文本能否成为作家在创作原创作品（无论是小说还是非小说）时可以借鉴的有益起点呢？让大语言模型处理样板文件是否能让作家将注意力集中在真正有创意的部分？\n显然，没有人可以代表所有作家说话，但让我来论证一下，从一份模糊且非原创的文本开始，并非创作原创作品的良方。如果你是一位作家，在你写出原创作品之前，你会写很多非原创的作品。花费在那些非原创作品上的时间和精力并没有白费；相反，我认为，恰恰是这些努力让你最终得以创作出原创作品。遣词造句、调整语序，这些练习教会了你如何运用文字传情达意。让学生写论文不仅仅是一种测试他们对材料掌握程度的方法；它还为他们提供了表达自己想法的经验。如果学生们写的都是千篇一律的文章，他们就永远无法掌握创作新颖内容所需的技能。\n而且，一旦你不再是一名学生，你就可以安全地使用大语言模型提供的模板，情况并非如此。表达自己想法的挣扎并不会在你毕业后消失——它可能在你每次开始起草新作品时发生。有时，只有在写作的过程中，你才能发现自己的原创想法。有人可能会说，大语言模型的输出看起来与人类作家的初稿并没有什么不同，但是，我认为这是一种肤浅的相似性。你的初稿并非清晰表达的非原创内容，而是一个尚未成熟的原创想法，伴随着你隐约的不满，因为你意识到它与你理想中的表达之间的差距。这就是在重写过程中指导你的东西，也是你在开始使用人工智能生成的文本时所缺少的东西之一。\n写作没有什么神奇或神秘之处，但它不仅仅是将现有文档放在不可靠的复印机上并按下打印按钮。在未来，我们有可能构建一个人工智能，它能够仅根据自己对世界的经验来写出好的散文。我们实现这一目标的那一天将是重要的——但那一天远远超出了我们的预测范围。与此同时，我们有理由问，拥有一个可以改写 Web 的东西有什么用呢？如果我们永远失去了对互联网的访问权限，并且必须将副本存储在空间有限的私有服务器上，那么像 ChatGPT 这样的大语言模型可能是一个好的解决方案，假设它可以防止捏造。但我们并没有失去对互联网的访问权限。所以，当你仍然拥有原件时，一张模糊的 JPEG 图像到底有多大用处呢？♦\n","permalink":"https://viitetrix.github.io/blog/posts/%E8%AF%91%E6%96%87-chatgpt%E4%BA%92%E8%81%94%E7%BD%91%E7%9A%84%E6%A8%A1%E7%B3%8A%E7%BC%A9%E5%BD%B1/","summary":"ChatGPT 像是互联网文本的一个模糊的 JPEG，它提供信息的转述而不是精确的引用，这种有损压缩虽然在表达上看似更智能，却牺牲了信息的准确性，并可能导致“幻觉”现象，即生成看似合理实则错误的内容。","title":"[译文] ChatGPT：互联网的模糊缩影"}]