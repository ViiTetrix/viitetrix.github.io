[{"content":"原文链接： 8 Ways AI Will Transform Journalism - Bloomberg 原文发布日期： January 10, 2025 at 9:00 AM UTC\n新闻行业正面临着下一个巨大的挑战。以下是有理由保持乐观和感到担忧的八个原因。\n![Karan Singh for Bloomberg](/blog/img/20250113/Karan Singh for Bloomberg.gif)\n我们记者天生就是一群疑心很重的人。总是不停地担心着某个人——可能是政府，可能是律师，可能是我们的同事，也可能是IT部门——会在某个地方对我们，或者我们的新闻稿做出什么糟糕的事情。\n到目前为止，21世纪只是让这种疑虑变本加厉。早在2006年，我担任《经济学人》杂志编辑时，做的第一批封面文章之一的标题就是《谁杀死了报纸？》。那时，互联网正在摧毁大多数大城市报纸悠闲自在的商业模式，这些报纸过去仰仗着它们在分类广告上的垄断地位过活。\n不过，现在回过头来看，那与其说是被暗杀，不如说是自取灭亡。太多优秀的媒体品牌轻信了科技界那些“传统媒体已死，内容就该免费”的鬼话。很快，它们就困在了这样一个恶性循环里：拼命追逐点击量，不断削减成本，然后眼睁睁地看着自己的生意一点点地落入科技巨头手中。\n但最终，理智还是占了上风，人们开始愿意为新闻付费，传统媒体也开始复苏。2012 年马克·汤普森加入《纽约时报》时，该报仅有 50 万数字订阅用户，之后他专注于推动这位“老太太”的报纸销售订阅服务，如今已拥有超过 1000 万付费用户。那些鼓吹“内容免费”、曾将众多业内精英引入歧途的“内容免费”的魔咒已经失效了；像The Information、Puck，还有The Free Press（别看名字叫“免费”，其实也要收费的）这些新晋挑战者都在确保人们迟早会掏钱。\n然而，正当优质新闻媒体逐渐适应了互联网和社交媒体之际，另一场规模更大的变革又来了：人工智能。\n人工智将会触及我们行业的核心——改变我们撰写和编辑新闻报道的方式。它将挑战我们，正如它正在挑战律师、编剧和会计师等其他知识工作者一样。\n这场变革究竟将如何展开？在我做出预测之前，先来一点个人的谦逊。当我成为《经济学人》的编辑时，我完全不知道一家名为 Twitter 的公司在十天前成立了；然而，九年后当我来到彭博社时，Twitter 在某种程度上已经成了世界上最大的“报纸”。所以，要对任何打包票的编辑（包括我自己在内）保持警惕。\n但我认为，我们在彭博社的新闻编辑室是一个很好的实验室，可以从中寻找关于这场变革可能如何发展的线索。部分原因是，我们比其他任何地方都更多地使用技术，包括早期版本的人工智能。在我们每天制作的 5000 篇报道中，超过三分之一的报道中都有某种形式的自动化。部分原因是，我们的受众非常接近未来要求苛刻的新闻消费者。我们的读者会依据我们报道的内容进行数百万美元的交易。因此，准确性和缺乏偏见对他们至关重要，但时间也是如此。我们的读者、观众和听众讨厌我们浪费他们的时间——正如我们将要看到的，节省时间是人工智能带来的一个核心优势。\n以下是人工智能目前已经可以做的两个例子。\n第一个是我们发布过一份报告，该报告显示了石油是如何从伊朗走私出去并在不同船只间转移的。为了避免被发现，相关人员无所不用其极——所以我们构建了一个算法，查看船舶的卫星图像，以检测是否有两艘船并排停靠。在 2020 年 1 月初到 2024 年 10 月 4 日之间，天空晴朗的 566 天里，我们发现了 2006 处这种可疑的并排排列——我们的记者可以对此进行调查。\n人工智能非常擅长模式识别——在大量的图像、文档或数据中进行筛选，当这些东西太多太模糊以至于人类无法处理时，人工智能可以帮助讲述故事。我们的数据新闻主管阿曼达·考克斯说，她最喜欢用来比喻大型语言模型的说法是“无限的实习生”。你并不总是完全信任他们带来的结果，但是，就像人类实习生一样，机器每天都在变得更好：从 2020 年的幼儿水平的智能，到至少在特定任务上已接近博士水平的智能，ChatGPT 及其同类产品的下一次迭代也是如此。\n当人工智能帮助他们揭露伊朗石油走私时，大多数记者都喜欢它。调查性新闻在新闻编辑室里很容易被接受。第二个例子有点难。在过去的一个月里，我们已经开始在彭博社软件上测试一些人工智能驱动的长篇报道的摘要。\n该软件阅读报道并生成三个要点。客户喜欢它——他们能迅速掌握任何文章的要点。记者们则更加怀疑。记者担心人们只会阅读摘要而不是他们的报道。对此，我的坦诚回答是：没错，读者很可能这样做。但你们是否更希望他们浪费时间浏览自己不感兴趣的段落呢？对我来说，这很明确：正确使用这些摘要，既可以帮助读者，也可以为编辑节省时间。\n那么，展望我们的实验室，我认为在人工智能时代会发生什么呢？以下是八个预测。\n首先，人工智能将更多地改变记者们的工作，而不是取代他们。 让我们看一个简单的例子——报道公司盈利公告。 当我刚到彭博社时，有一个“速度”团队，由反应敏捷的记者组成，他们专门快速产出头条新闻，力求比最接近的竞争对手快上几秒。 然后自动化出现了——电脑可以在几分之一秒内扫描公司的新闻稿。 人们担心自己的饭碗不保。 但是机器需要人。首先要告诉它们该寻找什么——对苹果公司股价而言，在中国售出的 iPhone 数量可能比实际收入更重要。 机器还需要人类来寻找和解读意外情况——例如，首席执行官的突然辞职可能意义重大，也可能无关紧要。\n我们仍然雇用大致相同数量的人来关注盈利情况，但是我们报道盈利的公司的数量以及围绕这些公告的报道深度都显著增加了。 而且，我认为，这项工作也变得更加有趣了；它不仅仅是快速打字，而是要弄清楚什么才是重要的。\n人工智能很可能也会发生同样的情况——倍增我们产生的内容数量。 例如，一个人手不足的分社可能没有足够的时间为读者提供关于叙利亚阿萨德政权垮台的解释性报道；但是，如果可以将你现有的四篇新闻稿通过算法处理呢？ 几秒钟内，你就会得到一份粗略的解释性报道草稿，供记者进一步完善。\n我们的读者、观众和听众讨厌我们浪费他们的时间——而节省时间是人工智能提供的关键价值所在。\n另一个显而易见的内容倍增器是自动翻译——更多的文章将触达更多的读者，更多大型全球组织的记者将能够用自己的语言写作。\n其次，突发新闻仍然非常有价值，但其价值持续的时间越来越短。 新闻的价值没有下降的迹象——政治变革如今在价值上可以与经济变革相提并论。 每次我们揭示华盛顿、巴黎或北京的政策转变时，都可以看到货币市场的波动。 但至关重要的是，这被认为是新闻的时效性正在持续缩短。 就重要的公告而言——比如就业数据——其时效性早已缩短到几分之一秒，我们的竞争对手通常是对冲基金，他们也在使用人工智能来尽可能快地查看数据。 就关于意外事件的新闻报道而言，例如收购或首席执行官辞职，这很难衡量，但我敢做一个不科学的猜测，在我于彭博社工作期间，价格变动所需的时间已经从几秒钟缩短到几毫秒。\n人工智能将使这一过程加速，并使其普及化。 这很大程度上取决于版权协议如何解决，但很有可能，随着越来越多的新闻出现，它将被立即吸收到像ChatGPT这样的机器中，这些机器考虑的不是一个市场——而是将其加到所谓的即时常识库中。 每个人，或者至少比现在更广泛的人群，都可以获取这些信息。\n第三，新闻报道依然具有巨大价值。 关于我目前提及的大部分要点，一个基本前提是我们需要高质量的新闻报道。人工智能摘要的质量完全取决于其所依据的新闻内容。而获取这些深度内容，仍然是人类记者不可替代的价值所在。机器无法说服一位内阁部长透露财政大臣刚刚辞职的消息；它无法邀请一位首席执行官共进午餐以获取内幕；它无法独立撰写一篇富有洞见的专栏文章，也无法巧妙地引导采访对象在节目中透露关键信息。\n至关重要的是，新闻编辑室依然需要实地记者。尤其是在全球格局日益复杂，我们不能再理所当然地认为印度尼西亚或印度等新兴国家会效仿西方的自由模式，且许多国家都在试图限制新闻报道的当下，我们需要的是那些在当地拥有深厚人脉的记者。\n第四，变革对编辑的影响可能大于对记者的影响。 将大多数编辑工作拆解为一系列技能。首先是管理记者团队：各位不会感到惊讶，我仍然自信地认为新闻编辑室需要像我这样的人。其次是约稿：我认为这仍然主要是一项人类技能——尽管在彭博社，我们已经利用人工智能来提示我们考虑报道某些新闻（例如，提示股价的异常波动或社交媒体上有关爆炸事件的讨论）。\n然而，一旦稿件完成并提交，当我们实际在屏幕上修改文字内容时，我认为人工智能工具将会越来越多地发挥作用，例如重构和重写草稿，以及进行事实核查等等。再次强调，我并非指《纽约客》那样精雕细琢的编辑工作。但许多新闻报道的形式相对公式化。\n以足球比赛的体育报道为例。设想一下，五年后，一位英国记者在皇权球场完成了一场比赛的报道，并将其发送给她在伦敦的编辑。一秒钟后，她和她的编辑都将收到一个经过人工智能编辑的版本：该版本将自动检查拼写和内部规范；系统会在存疑的表述旁提出疑问（例如，为什么记者声称利物浦控制了比赛，而实际上莱斯特的控球率达到了 51%？）；照片和视频片段也会被自动添加——以及指向四名莱斯特进球球员的相关链接。 说到这里，我的设想可能在很多方面都显得有些难以置信，尤其是对于那些关注足球的读者而言。但我认为大家可以看出，人工智能可能会更多地改变编辑的工作内容，而非记者的工作本身。\n第五，“搜索”模式将让位于“问答”模式。 随着像ChatGPT和Perplexity这样的大规模摘要工具不断吸纳海量新闻内容，它们正利用这些内容来构建直接的答案。当你向谷歌提问时，这种趋势已经显而易见。你得到的不再是冗长的链接列表，而是包含数句话，有时甚至接近一个段落的直接答案。我的同事，彭博社产品团队负责人克里斯·柯林斯认为，我们目前所知的搜索模式可能会消失。\n这将对那些依赖搜索广告以及依赖浏览量作为盈利方式的机构和个人产生巨大影响。目前，当读者点击一个链接时，发布者可能会从广告商那里获得几美分的收入。但是，当用户从搜索引擎（或者说，是答案引擎）获得越来越详尽的答案时，这些点击行为将会逐渐消失。\n这正是严肃的新闻出版物需要构建可持续的订阅业务——并投资于与忠实读者建立长期关系——的另一个重要原因。 这也提醒我们亟需厘清版权归属问题；我们需要我们的法院和立法机构更加明确哪些内容可以被免费使用，哪些不行。\n第六，在文本中解决幻觉问题比在视频或音频中更容易。 如果你和记者讨论人工智能，很可能会有人提到幻觉——也就是机器会编造故事或被诱导编造故事。人工智能必然会有一个试错的过程，而且不乏有人认为他们可以通过欺骗我们来获得商业或政治上的优势。我的预感是，在可预见的未来，主要的危险是 人工智能被用来生成虚假的视频或音频图像，这些图像会歪曲或恶意放大实际发生过的事件，而不是编造完全虚假的事件。\n这很大程度上与人类和机器之间的相互作用有关。几年前，我关注了我们的突发新闻团队处理地铁枪击事件的方式。他们最初是从社交媒体上得知有不好的事情发生。你可以看到电子信息交流迅速增加，但只有当他们有了信任的人类来源——在这种情况下，是现场的一位目击者——时，他们才会确认此事。\n相比之下，视频和音频更难确认。在地铁枪击事件中，社交媒体上出现了一张明显是死者的可怕照片。但这是真的吗？是编造的吗？要快速验证这一点更加困难。你必须将照片与地铁站的照片进行对比，检查它看是否有像素被移动过等等。也许人工智能会更容易找出欺诈性的音频和视频，但到目前为止，我看到的大多数例子都是越来越精巧的伪造品。\n获得真实的故事仍然是人类的关键所在。\n但有一点需要注意。就“假新闻”而言，值得注意的是，那些长期散布谎言的政权现在倾向于用虚假信息来掩盖真相，而不是坚持一个单一的谎言。例如，在过去，《真理报》会直接声明一个谎言——然后重复它。现在，当发生一些克里姆林宫不喜欢的事情（比如一架客机被击落或一场战斗失败）时，俄罗斯的机器人大军会产生多种可能的结果。主要目标是混淆视听。\n第七，个性化将变得更加现实。 这又是一个预感。个性化一直是数字新闻的圣杯。想象一下，如果你只得到你需要的新闻：你自己的个人报纸。到目前为止，这种情况只是相当笨拙地发生了。许多人不喜欢向新闻机构提供他们的详细信息——即使这样做似乎符合他们的利益。当有人向他们推荐内容时，一些读者会感到不安。他们担心自己会陷入意见隔阂。他们怀念那种意外发现的元素——你不知道自己会感兴趣的故事。这就像逛一家老式书店，你可以在那里随意浏览，偶然发现一本有趣的小说。这和被亚马逊推送建议的感觉是截然不同的。\n人工智能将开始破解这个难题。算法擅长找出你可能感兴趣的东西——发现人们自己看不到的模式。人工智能可以更轻松地与读者建立联系，它们比那些随机推荐的“为你准备的新闻”更有效，后者要么推送太多，要么让你错过热点。\n这种对内容的预测性个性化也伴随着阴暗面。预测我们可能喜欢园艺课程的算法，也可能引导一个刚刚被女朋友甩了的青少年去看关于自杀的视频。\n目前，社交媒体公司并不像我这样的编辑那样对其网络上的内容负责。由于像美国臭名昭著的“Section 230”这样的规则，科技巨头被视为更像是电话公司而不是媒体公司。他们对线路负责，但不对线路上的言论负责。\n这种说法已经站不住脚了，我预计随着人工智能的强大，会更加如此。几十年来，烟草公司一直躲在这样的论点背后：“不是他们的产品导致人们死亡，吸烟是个人选择的问题。”但最终这种辩护失败了。我认为科技巨头也会输掉这场战斗，尤其是有孩子的人都可以谈论他们产品的成瘾性。这引出了我的第八个也是最后一个预测：\n监管的脚步近了。 对于世界各地的政治家而言，人工智能终将变得过于复杂、过于强大、过于具有侵扰性，而且（如果你的国家不在美国）它“太美国化”了，他们不能再对其视而不见。上世纪 90 年代，美国政客为了鼓励创新，对年轻的互联网公司采取了放任态度。但如今，没人会觉得亚马逊、微软和 Facebook 这样的巨头需要被保护。情况恰恰相反。企业现在要做的远不止是遵守法律。只有当一家公司被认为在为社会做好事时，社会才会愿意赋予其有限责任等特权。许多公司，甚至整个行业，都可能失去这种社会特许权；你会从酷炫的创新者变成“为富不仁者”——正如西奥多·罗斯福（美国前总统）一个世纪前在推行反垄断法时称呼那些“强盗大亨” （指当时通过不正当手段攫取巨额财富的产业巨头）一样。\n我们现在就可以看到科技巨头正面临这样的境地。在美国，政治环境很复杂，因为美国立法者即使不喜欢这些科技巨头，仍然认为它们是美国在经济上领先于中国的原因之一。而在布鲁塞尔，这种顾虑会少得多——特别是当欧洲的政治家们意识到他们在人工智能领域已经落后了多远时。正如一位商人告诉我的那样，“美国创新，中国复制，欧洲监管。”\n这些就是我大致的预测。请再次记住，我可能又一次错过了“AI 版 Twitter”在十天前横空出世的机会。但这些基于些许见解的八个猜想，会把我们的世界，以及我自 1987 年以来赖以谋生的新闻行业带向何方呢？我认为，总的来说，我们可以保持一种既警惕又乐观的态度。\n之所以说警惕，是因为不难预见事情可能走向糟糕的一面——虚假内容泛滥成灾，新闻业在干预的政客和技术巨头之间左右为难，许多新闻编辑室的员工因为机器能代劳编辑工作而失业。最糟糕的情况下，在中国和俄罗斯等国家，政府可能会利用人工智能进一步打压独立新闻，追踪新闻来源，审查新闻内容，并散布他们自己精心炮制的虚假信息。\n但与此同时，乐观的曙光也开始显现。回想一下我们之前报道的伊朗船只事件，所有这些新技术将为我们提供更多识别模式，追究权势人物责任的手段。 过去，整个国家似乎都不受约束。但现在，政治家们随时随地都有可能被拍到视频，而且常常会做出一些蠢事。正如历史学家蒂莫西·斯奈德所说，“无论邪恶多么黑暗，总有一盏嘲讽的小灯笼在角落里闪耀。”\n我也乐观地认为，这一次我们的新闻行业对技术变革的准备更加充分了。与互联网和社交媒体兴起时相比，现在的编辑和出版商对人工智能更加警惕，更不愿意轻易放弃我们辛苦得来的内容，因此，对优质内容的追求将会更快到来。\n我说“这一次”，是因为我们常常会犯一个错误，认为我们是第一代经历技术变革的记者。事实上，本世纪到目前为止发生的事情（以及即将再次发生的事情）实际上只是一个旧故事的重演——一项新技术开启了一个疯狂和动荡的时期，然后逐渐回归理性。\n19 世纪初，蒸汽印刷机的出现使得大量印刷小册子和八卦报刊成为可能，这些报刊可以随意议论任何人。当时“廉价报刊” 的巨头是《纽约太阳报》，它迅速成为全球销量最大的报纸。该报最著名的一项调查报道宣称，他们借助一台巨大但又难以寻觅的望远镜，“发现”月球上居住着各种奇妙的生物，包括建造寺庙的半人半蝙蝠。\n但后来，情况开始逐渐好转。纽约人更愿意为有用的新闻付费，了解真实的世界；而新兴的消费品公司更倾向于在真实的故事旁边刊登他们的广告。新的报纸纷纷涌现。《经济学人》于 1843 年创刊，《纽约时报》和路透社均在 1851 年面世，《金融时报》于 1888 年，《华尔街日报》则于 1889 年问世。一场追求优质内容的浪潮就此展开。\n只要我们专注于原创报道，撰写那些当权者不希望我们发布，或能告诉我们关于世界新事物的故事，并且我们在这样做时不畏惧、不偏袒、不带偏见，我们就能做得很好。\n这篇随笔节选自我有幸在伦敦大学城市圣乔治分校发表的詹姆斯·卡梅隆纪念讲座。\n约翰·米克尔思韦特 是彭博社的总编辑。\n","permalink":"https://viitetrix.github.io/blog/posts/%E8%AF%91%E6%96%87-%E6%96%B0%E9%97%BB%E4%B8%9A%E5%B0%86%E5%A6%82%E4%BD%95%E9%80%82%E5%BA%94%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%97%B6%E4%BB%A3/","summary":"AI 将会给新闻业带来翻天覆地的变化，但这种变化更多的是“改变工作方式”，而不是直接“取代记者”。","title":"[译文] 新闻业将如何适应人工智能时代"},{"content":"原文链接：Generative AI exists because of the transformer\n原文发布日期：2023 年 9 月 12日\n在过去的几年里，我们在构建智能机器这条长达数十年的探索之路上实现了一个巨大的飞跃：大语言模型的出现。\n这项基于人脑模拟研究的技术催生了一个被称为“生成式 AI”的新领域——这种软件能够创作出与人类水平相当的、可信且复杂的文本、图像和计算机代码。\n全球的企业已经开始探索这项新技术的应用，相信它将颠覆媒体、金融、法律和专业服务以及教育等公共服务领域。大语言模型基于被称为 transformer 模型的一项科学突破，它由谷歌研究人员在 2017 年提出。\n“虽然我们一直都清楚我们 transformer 研究的突破性意义，但几年后，我们对其在医疗保健、机器人技术和安全等新领域的持久潜力感到振奋，它增强了人类的创造力等等。”谷歌高级研究员 Slav Petrov 说道，他致力于构建包括大语言模型在内的 AI 智能体。\n大语言模型备受推崇的优势——通过编写和分析文本来提高生产力的能力——也是它对人类构成威胁的原因。根据高盛的说法，它可能使大型经济体中相当于 3 亿名全职员工面临自动化，导致大范围失业。\n随着这项技术迅速融入我们的生活，理解大语言模型如何生成文本意味着理解为什么这些模型是如此多才多艺的认知引擎——以及它们还能帮助创造什么。\n要编写文本，大语言模型必须首先将单词转换成它们能理解的语言。\n首先，一段文字会被分解成 token ——这是可以被编码的基本单元。虽然 token 通常表示单词的一部分，但在本例中，为了简单起见，我们把每个完整的单词都当作一个 token。\n为了理解一个词的含义，以我们例子中的 work 为例，大语言模型首先会在海量的训练数据中观察它的上下文，并注意它的邻近词 。这些数据集基于互联网上发布的文本整理而成，新的大语言模型使用了数十亿个单词进行训练。\n最终，我们会得到一个庞大的单词集合，其中既包括在训练数据中与 work 一同出现的词，也包括那些没有和它一起出现的词。\n当大语言模型 处理 这些单词时，会生成一个向量（也就是一个数值列表），并根据每个单词在训练数据中与 work 的邻近程度来调整这个向量。这个向量被称为词嵌入。\n词嵌入可以包含数百个数值，每个数值代表单词含义的某个不同方面。就像你可以通过房子的特征——类型、位置、卧室数量、浴室数量、楼层数——来描述它一样，词嵌入中的数值量化了一个单词的语言特征。\n由于这些特征的提取方式，我们并不确切知道每个数值代表什么含义，但那些我们预期会以类似方式使用的单词，通常会有相似的词嵌入。\n例如，sea 和 ocean 这两个词可能不会在完全相同的上下文中使用（“all at ocean”不能直接替代“all at sea”，后者表示“不知所措”），但它们的含义非常相近，而词嵌入可以让我们量化这种相近的程度\n通过把每个词嵌入所代表的数百个数值缩减到只有两个，我们就能更清楚地看到这些单词之间的距离。\n我们可能会发现一些 代词 或 交通方式 的集群，而能够以这种方式量化单词，正是大语言模型生成文本的第一步。\n但这并不是大语言模型如此聪明的全部原因。真正让它们能够如此流畅地解析和写作的，是一种名为 transformer 的工具。它从根本上加速并增强了计算机理解语言的方式。\nTransformer 可以一次性处理整个序列 —— 无论是句子、段落还是整篇文章 —— 分析其所有组成部分，而不仅仅是单个词汇。\n这使得软件能够更好地捕捉上下文和模式，并更准确地翻译或生成文本。这种同步处理还使得大语言模型的训练速度更快，从而提高了它们的效率和扩展能力。\n2017 年 6 月，谷歌的八位 AI 研究人员首次发表了概述 transformer 模型的论文。他们这份 11 页的研究论文标志着生成式 AI 时代的开始。\nTransformer 架构的一个关键概念是自注意力机制。这使得 LLM 能够理解单词之间的关系。\n自注意力机制查看一段文本中的每个 token ，并判断哪些 token 对于理解其含义最重要。\n在 transformer 之前，最先进的 AI 翻译方法是循环神经网络 (RNNs)，它会扫描句子中的每个单词并按顺序处理。\n通过自注意力机制，transformer 可以同时计算句子中的所有单词。捕捉这种上下文使大语言模型具备了更复杂的语言解析能力。\n在这个例子中，通过一次性评估整个句子，transformer 能够理解 interest 在这里是用作名词，表示个人对政治的兴趣。\n如果我们调整一下句子\n模型会理解 interest 现在被用作金融意义上的“利息”。\n当我们将两个句子组合在一起时，由于模型对伴随文本的关注，它仍然能够识别每个单词的正确含义。\n在第一种用法中，模型最关注的是 no 和 in 。\n而在第二种用法中，模型最关注的是 rate 和 bank 。\n这个功能对于高级文本生成至关重要。如果没有它，在某些上下文中可以互换但在其他上下文中不能互换的单词可能会被错误地使用。\n简而言之，自注意力确保了在总结关于利率的句子时，不会误用 热情 一词。\n这种能力不仅仅局限于像“interest”这样具有多重含义的单词。\n在下面的句子中，自注意力机制能够计算出 it 最有可能指的是 dog 。\n如果我们改变句子，将 hungry 换成 delicious ，模型能够重新计算， it 现在最有可能指的是 bone 。\n自注意力机制对语言处理的好处随着规模的扩大而增加。它允许 LLM 从句子边界之外获取 上下文 ，使模型更好地理解单词的使用方式和时间。\nGPT-4 是全球最大且最先进的大语言模型之一，是 OpenAI 最新的 AI 模型。该公司称，该模型在美国律师资格考试、大学预修课程（AP）考试和 SAT 学术能力评估测试等多个学术和专业基准测试中展现出“人类水平的表现”。\nGPT-4 可以生成和接收大量文本：用户可以输入多达 25,000 个英文单词，这意味着它可以处理详细的财务文件、文学作品或技术手册。\n该产品的出现重塑了整个科技行业，包括谷歌、Meta 和微软这些支持 OpenAI 的科技巨头，以及一些小型初创公司，都在竞相抢占这一领域的领先地位。\n他们发布的知名大语言模型包括：谷歌的 PaLM 模型（为其聊天机器人 Bard 提供支持）、Anthropic 的 Claude 模型、Meta 的 LLaMA 模型，以及 Cohere 的 Command 模型等。\n虽然这些模型已经被许多企业采用，但其背后的一些公司正面临着围绕其使用从网络上抓取的受版权保护的文本、图像和音频的法律纠纷 。\n其原因在于，当前的大语言模型几乎都是基于整个英语互联网进行训练的——这使得它们比前几代模型强大得多。\n从这个庞大的词语和图像语料库中，这些模型学习如何识别模式，并最终预测下一个最佳词语。\n简单来说，在对提示进行 token 化和编码后，我们就得到一个数据块，它以机器能理解的方式表示我们的输入，包括词语之间的含义、位置和关系。\n该模型现在的目标是预测序列中的下一个单词，并重复执行此操作，直到生成完整的文本输出。\n为此，该模型会为每个 token 分配一个 概率分数，表示其成为序列中下一个单词的可能性。\n它会持续执行此操作，直到生成完整的文本输出。\n但是，这种孤立地预测下一个单词的方法（称为“贪婪搜索”）可能会产生问题。有时，尽管每个单独的 token 可能是下一个最佳匹配，但整个短语的相关性可能会降低。\n不一定总是错误的，但可能也不是你所期望的。\nTransformer 使用多种方法来解决这个问题并提高其输出质量。其中一个例子叫做 集束搜索 。\n它不只关注序列中的下一个单词，而是将更大范围的 token 集合作为一个整体来考虑其概率。\n通过集束搜索，该模型能够考虑多条路径并找到最佳选项。\n这产生了更好的结果，最终生成更连贯、更像人类的文本。\n但是事情并不总按计划进行。虽然文本可能看起来合理且连贯，但它并不总是与事实相符。大语言模型不是查找事实的搜索引擎；它们是模式识别引擎，猜测序列中的下一个最佳选项。\n由于这种固有的预测性质，大语言模型还可以在研究人员称之为“幻觉”的过程中捏造信息。它们可以生成虚构的数字、姓名、日期、引言——甚至是网络链接或整篇文章。\n大语言模型的使用者分享了一些示例。其中包括指向《金融时报》和彭博社不存在的新闻文章的链接，对研究论文的虚假引用，已出版书籍的错误作者，以及充满事实错误的传记。\n在纽约发生的一起引人注目的事件中，一位律师使用 ChatGPT 为一个案件撰写了一份简报。当辩方质疑这份报告时，他们发现其中充斥着虚构的司法意见和法律引文。“我不知道 ChatGPT 会编造案例，”这位律师后来在自己的法庭听证会上告诉法官。\n尽管研究人员表示幻觉永远不会被完全消除，但 Google、OpenAI 和其他公司正在努力通过一个称为“溯源”的过程来限制它们。这包括将大语言模型的输出与网络搜索结果进行交叉检查，并向用户提供引文以便他们进行验证。\n人类也被用来提供反馈和填补信息空白——这个过程被称为基于人类反馈的强化学习 (RLHF)——这进一步提高了输出的质量。但是，了解哪些查询可能会触发这些幻觉，以及如何预测和减少它们，仍然是一个巨大的研究挑战。\n尽管存在这些局限性，transformer 还是催生了大量尖端 AI 应用。除了为 Bard 和 ChatGPT 等聊天机器人提供支持外，它还驱动我们手机键盘上的自动完成和智能扬声器中的语音识别。\n然而，它真正的力量在于语言之外。它的发明者发现，transformer 模型可以识别和预测任何重复的主题或模式。从图像中的像素（使用 Dall-E、Midjourney 和 Stable Diffusion 等工具），到使用 GitHub CoPilot 等生成器的计算机代码。它甚至可以预测音乐中的音符和蛋白质中的 DNA，以帮助设计药物分子。\n几十年来，研究人员构建了专门的模型来总结、翻译、搜索和检索。transformer 将所有这些操作统一到一个单一的结构中，能够执行各种各样的任务。\n“采用这个预测下一个单词的简单模型，它……可以做任何事情，”AI 初创公司 Cohere 的首席执行官兼 transformer 论文的合著者 Aidan Gomez 说。\n现在，他们有一种“在整个互联网上训练”的模型，“输出的结果可以完成所有这些工作，并且比以前的任何东西都好”，他说。\n“这是这个故事的神奇之处。”\n","permalink":"https://viitetrix.github.io/blog/posts/%E8%AF%91%E6%96%87-%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9B%A0-transformer-%E8%80%8C%E5%AD%98%E5%9C%A8/","summary":"主要讲述了 Transformer 模型的诞生及其对生成式 AI 发展的关键推动作用，并探讨了其工作原理、应用领域以及潜在的挑战。","title":"[译文] 生成式人工智能因 Transformer 而存在"},{"content":"原文链接：ChatGPT Is a Blurry JPEG of the Web | The New Yorker\n原文发布日期：2023 年 2 月 9 日\nOpenAI 的聊天机器人提供解释，而谷歌提供引用。我们更倾向于哪个？\n2013 年，一家德国建筑公司的员工发现他们的施乐复印机有点古怪：复印的房屋平面图与原件存在细微但重要的差异。原始平面图上，三个房间的面积分别用矩形标为：14.13、21.11 和 17.42 平方米。然而，在复印件中，三个房间的面积都标着 14.13 平方米。该公司联系了计算机科学家 David Kriesel 来调查这个匪夷所思的结果。之所以需要计算机科学家，是因为现代施乐复印机早已不用上世纪六十年代流行的物理静电复印技术，而是先将文件进行数字扫描，再打印出图像文件。再加上几乎所有数字图像文件都会为了节省空间而被压缩，谜底呼之欲出。\n文件压缩分两步：编码，将文件转换成更紧凑的格式；解码，将文件还原。如果还原的文件与原件完全一致，就是无损压缩，信息没有丢失。反之，如果还原的文件只是原件的近似版本，则是有损压缩，部分信息已丢失且无法恢复。无损压缩常用于文本文件和计算机程序，因为在这些领域，哪怕一个字符出错都可能造成灾难性后果。而在可以接受精度损失的情况下，有损压缩则常用于照片、音频和视频。多数时候，图片、歌曲或电影没有被完美还原，我们并不会察觉。只有当文件被极度压缩时，保真度的损失才会变得明显。我们会注意到所谓的压缩失真：比如极度压缩的 JPEG、MPEG 图像的模糊，或低比特率 MP3 听起来会很刺耳。\n施乐复印机使用一种叫 JBIG2 的有损压缩格式，主要用于黑白图像。为了节省空间，复印机会识别出图像中相似的区域，并只保存一份副本；解压时，它会重复使用该副本以重建图像。结果发现，复印机判定标注房间面积的数字足够相似，只需存储其中一个 “ 14.13 ” 并在打印平面图时重复使用。\n施乐复印机使用有损而非无损压缩格式，这本身不是问题。问题在于复印机悄无声息地降低了图像质量，且其压缩失真不易被察觉。如果复印机打印出的只是模糊的文本，大家自然会明白它并非原件的精确复制。而问题恰恰在于，复印机打印的数字清晰可读但却有误，这让复印件看似精确，实则不然。（2014 年，施乐发布了一个补丁来修复此问题。）\n我认为，在讨论 OpenAI 的 ChatGPT 及其他类似程序（AI 研究人员称之为大语言模型）时，有必要牢记施乐复印机的这段往事。复印机和大语言模型之间的相似性或许并不显而易见，但请思考以下场景：假设你即将永远无法访问互联网，为此，你计划将网络上的所有文本压缩保存到私有服务器。不幸的是，你的服务器只有所需空间的百分之一，如果想存储所有内容，就不能使用无损压缩算法。于是你编写了一个有损算法，该算法能识别文本中的统计规律，并将其存储为一种特殊的文件格式。由于你拥有几乎无限的计算能力，你的算法可以识别出极其细微的统计规律，从而实现 100:1 的惊人压缩比。\n现在，失去互联网访问权限似乎也不是那么可怕了，因为你已经在服务器上存储了网络上的所有信息。唯一的问题是，由于文本被高度压缩，你不能通过精确搜索来查找信息，因为你永远无法得到精确匹配，毕竟存储的并非字词本身。为了解决这个问题，你创建了一个界面，它接收提问形式的查询，并给出概括服务器上信息要旨的回答。\n我所描述的听起来很像 ChatGPT，或者说很像任何其他大语言模型。不妨把 ChatGPT 看作是网络上所有文本的一张模糊的 JPEG 图像。它保留了网络上的大部分信息，正如 JPEG 图像保留了高分辨率图像的大部分信息，但是如果你寻找的是一个精确的比特序列，你是找不到的；你得到的永远只是一个近似值。不过，由于这个近似值以语法文本的形式呈现，而这正是 ChatGPT 所擅长的，因此它通常可以接受。你看到的仍然是一张模糊的 JPEG 图像，但其模糊的方式并不会使整个图片看起来失真。\n这种类比不仅有助于理解 ChatGPT 如何通过变换措辞来重新组织网络信息，还可以解释大语言模型（如 ChatGPT）为何容易产生“幻觉”，即给出毫无意义的事实性回答。这些幻觉是压缩失真，但就像施乐复印机产生的错误标签一样，它们看似合理，只有与原件（即网络信息或我们的已有知识）比对后才能识别。这么一想，出现幻觉也就不足为奇了；如果一个压缩算法在丢弃了 99% 的原始内容后还能重建文本，那么其生成的很多内容完全是虚构的，也就在意料之中了。\n当我们意识到有损压缩算法常用的技术是“插值”时，这种类比就更说得通了——插值，就是通过查看空白两侧的内容来推测缺失的信息。当图像程序显示照片，需要重建压缩时丢失的像素时，它会查看附近的像素并计算平均值。这正是 ChatGPT 在收到指令时所做的，例如让它用《独立宣言》的风格描述在烘干机里丢了一只袜子，它会在“词汇空间”中取两个点，并生成填补其间空白的文本（“当人类历史的进程中，一个人有必要将其衣物与其配偶分离，以维护其清洁和秩序……”）。ChatGPT 极擅长这种插值，以至于人们觉得很有趣：这就像给文字而不是图片用了“模糊”工具，而且玩得不亦乐乎。\n鉴于 ChatGPT 等大语言模型常被吹捧为人工智能的前沿技术，将其比作有损文本压缩算法，听起来似乎是轻视，至少令人感到泄气。我承认这种观点有助于纠正人们将大语言模型拟人化的倾向，但这种压缩类比还有另一方面值得思考。自 2006 年起，一位名叫 Marcus Hutter 的 AI 研究员设立了一项奖金——“人类知识压缩奖”，又称“Hutter 奖”——奖励给那些能将维基百科一个特定的一千兆字节大小的快照进行无损压缩、且压缩后文件大小小于之前获奖者的人。你可能遇到过用 zip 格式压缩的文件。zip 格式能将 Hutter 设定的一千兆字节文件压缩到约三百兆字节，而最近的获奖者将其压缩到了 115 兆字节。这不仅仅是一次压缩练习。Hutter 认为，更好的文本压缩将有助于创造人类水平的人工智能，部分原因在于理解文本才能实现最大程度的压缩。\n为了说明压缩和理解之间的关系，想象你有一个文本文件，其中有一百万个加减乘除的例子。尽管任何压缩算法都能减小这个文件的大小，但要实现最大压缩比，可能需要推导出算术原理，然后编写一个计算器程序代码。使用计算器，你不仅可以完美重建这一百万个例子，还可以重建你未来可能遇到的任何其他算术题。同样的逻辑也适用于压缩维基百科的一个切片。如果压缩程序知道力等于质量乘以加速度，它就可以在压缩物理学页面时丢弃很多词语，因为它可以重建这些词语。同样，程序对供求关系了解得越多，它在压缩经济学页面时就能丢弃越多词语，以此类推。\n大语言模型识别文本中的统计规律。对网络文本的任何分析都会表明，“供应短缺”之类的短语经常出现在“价格上涨”之类的短语附近。一个结合了这种相关性的聊天机器人，在被问及供应短缺的影响时，可能会回答价格上涨。如果一个大语言模型汇集了大量经济学术语之间的相关性——多到足以对各种问题给出合理的回答——我们是否能说它真的理解了经济学理论？由于种种原因，像 ChatGPT 这样的模型没有资格获得 Hutter 奖，原因之一是它们不能精确重建原文，也就是说，它们并非无损压缩。但是，难道它们的有损压缩就没有展现出 AI 研究人员感兴趣的那种真正的理解吗？\n让我们回到算术的例子。如果你让 GPT-3（ChatGPT 基于的大语言模型）做两个数的加减法，当数字只有两位数时，它几乎总能给出正确答案。但随着数字变大，它的准确率会显著下降，到五位数时降至 10%。GPT-3 给出的大多数正确答案在网络上都找不到——例如，没有多少网页包含“245 + 821”——所以它并非简单记忆。但是，尽管消化了大量信息，它也未能推导出算术原理。仔细检查 GPT-3 的错误答案会发现，它在做算术时并没有进位。网络上当然有关于进位的解释，但 GPT-3 无法将这些解释纳入其中。GPT-3 对算术例子的统计分析使它能够生成真实算术的粗浅近似，但也仅此而已。\n既然 GPT-3 在小学水平的科目上都会失败，那我们该如何解释它有时在撰写大学水平论文时表现出色呢？尽管大语言模型经常产生幻觉，但在“清醒”时，它们似乎真的理解经济学理论等学科。也许算术是个特例，大语言模型并不擅长。那么在加减法之外的领域，文本中的统计规律是否真的对应着对现实世界的真实认知呢？\n我认为有一个更简单的解释。想象一下，如果 ChatGPT 是一种无损算法，它会是什么样子？如果是这样，它总是会从相关网页上逐字引用来回答问题。我们大概只会把它当作传统搜索引擎的略微改进版，而不会对它如此印象深刻。ChatGPT 改述而非逐字引用网络内容，这使它看起来像一个学生在用自己的话表达思想，而不是简单复述读过的内容；这造成了 ChatGPT 理解材料的假象。对人类学生来说，死记硬背并非真正学习的标志，所以 ChatGPT 不能精确引用网络内容，恰恰让我们以为它学到了什么。对于词语序列，有损压缩看起来比无损压缩更智能。\n人们已经提出了大语言模型的许多用途。将它们看作模糊的 JPEG 图像，可以帮助我们评估它们适合做什么，不适合做什么。我们不妨考虑几种场景。\n大语言模型能否取代传统搜索引擎？要让我们对它们有信心，就需要知道它们没有被灌输政治宣传和阴谋论——我们需要知道 JPEG 图像截取的是网络上的正确部分。但是，即使一个大语言模型只包含我们想要的信息，仍然存在模糊性的问题。有一种模糊性是可以接受的，那就是用不同的词语重新表述信息。还有一种模糊性是彻头彻尾的捏造，当我们需要事实时，这是不可接受的。目前尚不清楚在技术上能否做到在消除不可接受的模糊性的同时，保留可接受的模糊性，但我预计我们很快就会找到答案。\n即便可以限制大语言模型进行捏造，我们是否该用它们来生成网络内容？只有当我们的目标是重新包装网络上已有的信息时，这才说得通。有些公司专门干这个——我们通常称它们为“内容农场”。也许大语言模型的模糊性对它们有用，可以作为避免侵犯版权的一种方式。但总的来说，我认为任何对内容农场有利的事情，对搜索信息的人都是不利的。这种“信息再加工”的兴起，使得我们更难在网上找到所需内容；大语言模型生成的文本在网上发布得越多，网络本身就变得越模糊。\n关于 OpenAI 即将推出的 ChatGPT 继任者 GPT-4 的信息很少。但我将做一个预测：在收集用于训练 GPT-4 的大量文本时，OpenAI 的人会尽力排除 ChatGPT 或任何其他大语言模型生成的材料。如果真是这样，这将无意中证实大语言模型和有损压缩之间的类比是有用的。重复保存 JPEG 会产生更多的压缩失真，因为每次都会丢失更多信息。这相当于过去反复复印的数字版本，图像质量只会越来越差。\n实际上，衡量大语言模型质量的一个有用标准可能是，公司是否愿意使用其生成的文本作为新模型的训练材料。如果 ChatGPT 的输出对 GPT-4 来说不够好，我们就可以认为它对我们来说也不够好。反之，如果一个模型开始生成非常好的文本，好到可以用来训练新模型，那么这应该让我们对该文本的质量充满信心。（我怀疑这样的结果需要在构建这些模型的技术上取得重大突破。）而一旦模型生成的输出质量与其输入不相上下时，有损压缩的比喻就不再适用了。\n大语言模型能否帮助人们创作原创作品？要回答这个问题，我们需要明确这个问题的含义。有一种艺术流派被称为施乐艺术，或复印艺术，其中艺术家将复印机的独特属性用作创作工具。类似于 ChatGPT 的复印机肯定可以做到这一点，所以，从这个意义上说，答案是肯定的。但我认为没有人会声称复印机已成为艺术创作中必不可少的工具；绝大多数艺术家在他们的创作过程中不使用它们，也没有人争辩说他们在做出这种选择时会让自己处于不利地位。\n因此，让我们假设我们不是在谈论一种类似于施乐艺术的新型写作流派。鉴于这一规定，大语言模型生成的文本能否成为作家在创作原创作品（无论是小说还是非小说）时可以借鉴的有益起点呢？让大语言模型处理样板文件是否能让作家将注意力集中在真正有创意的部分？\n显然，没有人可以代表所有作家说话，但让我来论证一下，从一份模糊且非原创的文本开始，并非创作原创作品的良方。如果你是一位作家，在你写出原创作品之前，你会写很多非原创的作品。花费在那些非原创作品上的时间和精力并没有白费；相反，我认为，恰恰是这些努力让你最终得以创作出原创作品。遣词造句、调整语序，这些练习教会了你如何运用文字传情达意。让学生写论文不仅仅是一种测试他们对材料掌握程度的方法；它还为他们提供了表达自己想法的经验。如果学生们写的都是千篇一律的文章，他们就永远无法掌握创作新颖内容所需的技能。\n而且，一旦你不再是一名学生，你就可以安全地使用大语言模型提供的模板，情况并非如此。表达自己想法的挣扎并不会在你毕业后消失——它可能在你每次开始起草新作品时发生。有时，只有在写作的过程中，你才能发现自己的原创想法。有人可能会说，大语言模型的输出看起来与人类作家的初稿并没有什么不同，但是，我认为这是一种肤浅的相似性。你的初稿并非清晰表达的非原创内容，而是一个尚未成熟的原创想法，伴随着你隐约的不满，因为你意识到它与你理想中的表达之间的差距。这就是在重写过程中指导你的东西，也是你在开始使用人工智能生成的文本时所缺少的东西之一。\n写作没有什么神奇或神秘之处，但它不仅仅是将现有文档放在不可靠的复印机上并按下打印按钮。在未来，我们有可能构建一个人工智能，它能够仅根据自己对世界的经验来写出好的散文。我们实现这一目标的那一天将是重要的——但那一天远远超出了我们的预测范围。与此同时，我们有理由问，拥有一个可以改写 Web 的东西有什么用呢？如果我们永远失去了对互联网的访问权限，并且必须将副本存储在空间有限的私有服务器上，那么像 ChatGPT 这样的大语言模型可能是一个好的解决方案，假设它可以防止捏造。但我们并没有失去对互联网的访问权限。所以，当你仍然拥有原件时，一张模糊的 JPEG 图像到底有多大用处呢？♦\n","permalink":"https://viitetrix.github.io/blog/posts/%E8%AF%91%E6%96%87-chatgpt%E4%BA%92%E8%81%94%E7%BD%91%E7%9A%84%E6%A8%A1%E7%B3%8A%E7%BC%A9%E5%BD%B1/","summary":"ChatGPT 像是互联网文本的一个模糊的 JPEG，它提供信息的转述而不是精确的引用，这种有损压缩虽然在表达上看似更智能，却牺牲了信息的准确性，并可能导致“幻觉”现象，即生成看似合理实则错误的内容。","title":"[译文] ChatGPT：互联网的模糊缩影"}]